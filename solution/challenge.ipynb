{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "El archivo [SOLUTION.md](SOLUTION.md) contiene la explicaci√≥n detallada del paso apaso de la soluci√≥n\n",
    "\n",
    "## Ambiente de desarrollo\n",
    "\n",
    "Se utiliza make para desarrollar el proyecto. Puedes ver las opciones disponibles en el archivo [Makefile](Makefile).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import emoji\n",
    "import random\n",
    "import orjson\n",
    "import unicodedata\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Iterable\n",
    "\n",
    "# Local libraries - ensure project root is in sys.path before local imports\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.common.utils import twitter_schema  # noqa: E402\n",
    "from src.common.performance import profile_performance, profile_detailed  # noqa: E402\n",
    "\n",
    "file_path = \"../farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analisis\n",
    "\n",
    "Se realiza un analisis de los datos para comprender su estructura y posibles problemas de calidad. ver detalle en la secci√≥n 2 del archivo [SOLUTION.md](../SOLUTION.md) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_twitter_nuances(file_path, sample_size=20000):\n",
    "    print(f\"--- üïµÔ∏è‚Äç‚ôÇÔ∏è Deep Dive Analysis: {file_path} ---\")\n",
    "\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= sample_size:\n",
    "                    break\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Archivo no encontrado.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # ==========================================\n",
    "    # 1. AN√ÅLISIS DE TRUNCAMIENTO (Extended Mode)\n",
    "    # ==========================================\n",
    "    print(\"\\n[1] AN√ÅLISIS DE TRUNCAMIENTO\")\n",
    "\n",
    "    # Verificar si existen claves nativas de la API v1.1\n",
    "    has_truncated_key = \"truncated\" in df.columns\n",
    "    has_extended_tweet = \"extended_tweet\" in df.columns\n",
    "\n",
    "    if has_truncated_key:\n",
    "        truncated_count = df[\"truncated\"].sum() if df[\"truncated\"].dtype == bool else 0\n",
    "        print(\n",
    "            f\" - Tweets marcados como 'truncated': {truncated_count} ({truncated_count / len(df):.2%})\"\n",
    "        )\n",
    "\n",
    "        if has_extended_tweet:\n",
    "            extended_count = df[\"extended_tweet\"].notnull().sum()\n",
    "            print(f\" - Tweets con objeto 'extended_tweet' disponible: {extended_count}\")\n",
    "    else:\n",
    "        print(\n",
    "            \" - La clave 'truncated' NO existe en este dataset (probablemente ya fue procesado/aplanado).\"\n",
    "        )\n",
    "\n",
    "    # Verificar visualmente si el contenido parece cortado\n",
    "    # Los tweets truncados suelen terminar en \"...\" o un enlace t.co\n",
    "    df[\"ends_with_ellipsis\"] = df[\"content\"].astype(str).str.strip().str.endswith(\"‚Ä¶\")\n",
    "    suspicious_truncation = df[\"ends_with_ellipsis\"].sum()\n",
    "    print(f\" - Tweets que terminan visualmente en '‚Ä¶': {suspicious_truncation}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. AN√ÅLISIS DE RETWEETS (Duplicidad)\n",
    "    # ==========================================\n",
    "    print(\"\\n[2] AN√ÅLISIS DE RETWEETS\")\n",
    "\n",
    "    # Detectar RTs\n",
    "    # Opci√≥n A: Clave 'retweeted_status' (Standard API)\n",
    "    if \"retweeted_status\" in df.columns:\n",
    "        rts_count = df[\"retweeted_status\"].notnull().sum()\n",
    "        print(f\" - Detectados por metadato 'retweeted_status': {rts_count}\")\n",
    "\n",
    "    # Opci√≥n B: Texto empieza con \"RT @\"\n",
    "    df[\"is_rt_text\"] = df[\"content\"].astype(str).str.startswith(\"RT @\")\n",
    "    rts_text_count = df[\"is_rt_text\"].sum()\n",
    "    print(\n",
    "        f\" - Detectados por texto ('RT @...'): {rts_text_count} ({rts_text_count / len(df):.2%})\"\n",
    "    )\n",
    "\n",
    "    if rts_text_count > 0:\n",
    "        print(\n",
    "            \"   -> CONCLUSI√ìN: Los Retweets est√°n presentes. Q2 y Q3 estar√°n inflados por repetici√≥n.\"\n",
    "        )\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. MENCIONES: TEXTO VS METADATA (Q3)\n",
    "    # ==========================================\n",
    "    print(\"\\n[3] COMPARATIVA DE MENCIONES (Q3)\")\n",
    "\n",
    "    # Funci√≥n para extraer menciones con Regex (enfoque ingenuo)\n",
    "    def extract_regex_mentions(text):\n",
    "        return set(re.findall(r\"@(\\w+)\", str(text)))\n",
    "\n",
    "    # Funci√≥n para extraer menciones de Metadata (enfoque robusto)\n",
    "    def extract_meta_mentions(mentions_list):\n",
    "        if not isinstance(mentions_list, list):\n",
    "            return set()\n",
    "        return set(\n",
    "            m.get(\"username\")\n",
    "            for m in mentions_list\n",
    "            if isinstance(m, dict) and m.get(\"username\")\n",
    "        )\n",
    "\n",
    "    # Aplicar comparativa en una muestra peque√±a para velocidad\n",
    "    sample_df = df.head(1000).copy()\n",
    "\n",
    "    sample_df[\"regex_mentions\"] = sample_df[\"content\"].apply(extract_regex_mentions)\n",
    "    sample_df[\"meta_mentions\"].fillna(\"\", inplace=True)\n",
    "    sample_df[\"meta_mentions\"] = sample_df[\"mentionedUsers\"].apply(\n",
    "        extract_meta_mentions\n",
    "    )\n",
    "\n",
    "    # Buscar discrepancias\n",
    "    # Casos donde Metadata tiene ALGO pero Regex NO (Menciones invisibles/Reply)\n",
    "    sample_df[\"hidden_mentions\"] = sample_df.apply(\n",
    "        lambda x: x[\"meta_mentions\"] - x[\"regex_mentions\"], axis=1\n",
    "    )\n",
    "    hidden_count = sample_df[sample_df[\"hidden_mentions\"].astype(bool)].shape[0]\n",
    "\n",
    "    # Casos donde Regex tiene ALGO pero Metadata NO (Falsos positivos, emails, usuarios suspendidos)\n",
    "    sample_df[\"fake_mentions\"] = sample_df.apply(\n",
    "        lambda x: x[\"regex_mentions\"] - x[\"meta_mentions\"], axis=1\n",
    "    )\n",
    "    fake_count = sample_df[sample_df[\"fake_mentions\"].astype(bool)].shape[0]\n",
    "\n",
    "    print(f\"An√°lisis sobre {len(sample_df)} registros:\")\n",
    "    print(\n",
    "        f\" - Casos donde Metadata detecta usuarios que Regex NO ve (Hidden/Reply): {hidden_count}\"\n",
    "    )\n",
    "    if hidden_count > 0:\n",
    "        example = sample_df[sample_df[\"hidden_mentions\"].astype(bool)].iloc[0]\n",
    "        print(\n",
    "            f\"   Ejemplo Hidden -> Texto: '{example['content'][:50]}...' | Meta: {example['meta_mentions']}\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\" - Casos donde Regex detecta '@' que NO son usuarios v√°lidos en Metadata: {fake_count}\"\n",
    "    )\n",
    "    if fake_count > 0:\n",
    "        example = sample_df[sample_df[\"fake_mentions\"].astype(bool)].iloc[0]\n",
    "        print(\n",
    "            f\"   Ejemplo Falso Positivo -> Texto: '{example['content'][:50]}...' | Regex: {example['regex_mentions']}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def advanced_analysis(file_path):\n",
    "    print(f\"--- üî¨ An√°lisis Forense Avanzado: {file_path} ---\")\n",
    "\n",
    "    usernames_raw = []\n",
    "    usernames_normalized = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    tweet = json.loads(line)\n",
    "                    u = tweet.get(\"user\", {}).get(\"username\")\n",
    "                    if u:\n",
    "                        usernames_raw.append(u)\n",
    "                        # Normalizaci√≥n NFKC + Lowercase\n",
    "                        usernames_normalized.append(\n",
    "                            unicodedata.normalize(\"NFKC\", u).lower()\n",
    "                        )\n",
    "                except Exception:\n",
    "                    continue\n",
    "    except FileNotFoundError:\n",
    "        print(\"Archivo no encontrado\")\n",
    "        return\n",
    "\n",
    "    # 1. CHECK DE CASE SENSITIVITY Y UNICODE\n",
    "    unique_raw = len(set(usernames_raw))\n",
    "    unique_norm = len(set(usernames_normalized))\n",
    "\n",
    "    print(\"\\n[1] INTEGRIDAD DE ENTIDADES (Usernames)\")\n",
    "    print(f\" - Usuarios √∫nicos (Crudo): {unique_raw}\")\n",
    "    print(f\" - Usuarios √∫nicos (Normalizado + Lower): {unique_norm}\")\n",
    "    diff = unique_raw - unique_norm\n",
    "    if diff > 0:\n",
    "        print(\n",
    "            f\" ‚ö†Ô∏è ALERTA: Se detectaron {diff} duplicados por falta de normalizaci√≥n/may√∫sculas.\"\n",
    "        )\n",
    "        print(\" -> ACCI√ìN: Es MANDATORIO aplicar .lower() y unicodedata.\")\n",
    "    else:\n",
    "        print(\n",
    "            \" -> OK: No se detectaron colisiones, pero es buena pr√°ctica implementarlo.\"\n",
    "        )\n",
    "\n",
    "    # 2. DETECCI√ìN DE BOTS (OUTLIERS Q1)\n",
    "    print(\"\\n[2] DISTRIBUCI√ìN DE ACTIVIDAD (Q1)\")\n",
    "    counts = Counter(usernames_raw)\n",
    "    top_5 = counts.most_common(5)\n",
    "\n",
    "    df_activity = pd.Series(list(counts.values()))\n",
    "    p99 = df_activity.quantile(0.99)\n",
    "    max_tweets = df_activity.max()\n",
    "\n",
    "    print(f\" - Top 5 Usuarios m√°s activos:\\n   {top_5}\")\n",
    "    print(f\" - El 99% de usuarios tiene menos de {p99:.0f} tweets.\")\n",
    "    print(f\" - El usuario #1 tiene {max_tweets} tweets.\")\n",
    "\n",
    "    if max_tweets > (p99 * 10):\n",
    "        print(\n",
    "            f\" ‚ö†Ô∏è ALERTA: El usuario top tiene una actividad {max_tweets / p99:.1f}x mayor al promedio.\"\n",
    "        )\n",
    "        print(\n",
    "            \" -> OBSERVACI√ìN: Probable Bot. Documentar en SOLUTION.md que esto sesga los resultados.\"\n",
    "        )\n",
    "\n",
    "    # 3. EMPATES (TIE-BREAKING)\n",
    "    print(\"\\n[3] RIESGO DE EMPATES EN EL CORTE\")\n",
    "    # Ver si hay muchos usuarios con el mismo conteo en el borde del top 10\n",
    "    counts_values = list(counts.values())\n",
    "    counts_freq = Counter(counts_values)\n",
    "\n",
    "    # Imaginemos que el corte del top 10 es alrededor de X tweets\n",
    "    sorted_counts = sorted(counts_values, reverse=True)\n",
    "    if len(sorted_counts) > 10:\n",
    "        val_at_10 = sorted_counts[9]  # El valor del d√©cimo lugar\n",
    "        users_at_cutoff = counts_freq[val_at_10]\n",
    "        print(f\" - Valor de corte (Puesto #10): {val_at_10} tweets\")\n",
    "        print(f\" - Cu√°ntos usuarios tienen exactamente ese valor: {users_at_cutoff}\")\n",
    "\n",
    "        if users_at_cutoff > 1:\n",
    "            print(\" ‚ö†Ô∏è ALERTA CR√çTICA: Hay EMPATE en el puesto #10.\")\n",
    "            print(\n",
    "                \" -> ACCI√ìN: Tu c√≥digo DEBE tener un criterio de desempate (ej: alfab√©tico) o los tests fallar√°n aleatoriamente.\"\n",
    "            )\n",
    "\n",
    "\n",
    "def discover_anomalies(file_path, sample_size=15000):\n",
    "    print(f\"--- Iniciando An√°lisis Profundo: {file_path} (n={sample_size}) ---\")\n",
    "\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i < sample_size:\n",
    "                    data.append(line)\n",
    "                else:\n",
    "                    r = random.randint(0, i)\n",
    "                    if r < sample_size:\n",
    "                        data[r] = line\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo {file_path} no existe.\")\n",
    "        return\n",
    "\n",
    "    parsed = []\n",
    "    corrupt_lines = 0\n",
    "    for line in data:\n",
    "        try:\n",
    "            parsed.append(json.loads(line))\n",
    "        except Exception:\n",
    "            corrupt_lines += 1\n",
    "\n",
    "    if corrupt_lines:\n",
    "        print(f\"[ALERTA] Se detectaron {corrupt_lines} l√≠neas corruptas en la muestra.\")\n",
    "\n",
    "    # Normalizaci√≥n para an√°lisis\n",
    "    df_anom = pd.json_normalize(parsed)\n",
    "\n",
    "    print(\"\\n1. INTEGRIDAD DE COLUMNAS CLAVE\")\n",
    "    columns_to_check = [\"date\", \"content\", \"mentionedUsers\", \"user.username\", \"user.id\"]\n",
    "    for col in columns_to_check:\n",
    "        if col in df_anom.columns:\n",
    "            types = df_anom[col].apply(lambda x: type(x).__name__).value_counts()\n",
    "            nulls = df_anom[col].isnull().sum()\n",
    "            print(f\"- '{col}': {len(types)} tipos detectados. Nulos: {nulls}\")\n",
    "        else:\n",
    "            print(f\"- [ERROR] Columna '{col}' NO encontrada.\")\n",
    "\n",
    "    print(\"\\n2. AN√ÅLISIS DE USUARIOS (Q1 & Q3)\")\n",
    "    # Verificar si un username tiene m√∫ltiples IDs (cambio de handle)\n",
    "    user_consistency = df_anom.groupby(\"user.username\")[\"user.id\"].nunique()\n",
    "    inconsistent = user_consistency[user_consistency > 1]\n",
    "    print(f\"- Usuarios con m√°s de un ID: {len(inconsistent)}\")\n",
    "\n",
    "    # Calcular potencial de String Interning\n",
    "    total_names = len(df_anom[\"user.username\"])\n",
    "    unique_names = df_anom[\"user.username\"].nunique()\n",
    "    print(\n",
    "        f\"- Ratio de Repetici√≥n de Usernames: {total_names / unique_names:.2f}x (Alto ratio justifica sys.intern)\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n3. AN√ÅLISIS DE MENCIONES (Q3)\")\n",
    "    # Validar estructura interna de mentionedUsers\n",
    "    mentions_data = df_anom[\"mentionedUsers\"].dropna()\n",
    "    has_nested_nulls = mentions_data.apply(\n",
    "        lambda x: any(m.get(\"username\") is None for m in x if isinstance(m, dict))\n",
    "    ).sum()\n",
    "    print(f\"- Registros con listas de menciones v√°lidas: {len(mentions_data)}\")\n",
    "    print(f\"- Listas con objetos internos nulos: {has_nested_nulls}\")\n",
    "\n",
    "    print(\"\\n4. AN√ÅLISIS DE EMOJIS COMPLEJOS (Q2)\")\n",
    "    all_emojis_anom = []\n",
    "    complex_count_anom = 0\n",
    "    for txt in df_anom[\"content\"].dropna():\n",
    "        # emoji_list devuelve informaci√≥n detallada de cada emoji\n",
    "        found = emoji.emoji_list(txt)\n",
    "        for e in found:\n",
    "            char = e[\"emoji\"]\n",
    "            all_emojis_anom.append(char)\n",
    "            # Si tiene m√°s de un componente unicode o caracteres especiales de uni√≥n\n",
    "            if len(char) > 1 or \"\\u200d\" in char:\n",
    "                complex_count_anom += 1\n",
    "\n",
    "    print(f\"- Total emojis detectados: {len(all_emojis_anom)}\")\n",
    "    print(f\"- Emojis complejos (ZWJ/Multi-char): {complex_count_anom}\")\n",
    "    if all_emojis_anom:\n",
    "        print(f\"- Top 3 Emojis en muestra: {Counter(all_emojis_anom).most_common(3)}\")\n",
    "\n",
    "    print(\"\\n5. VALORES EXTREMOS Y FECHAS\")\n",
    "    df_anom[\"date_parsed\"] = pd.to_datetime(df_anom[\"date\"], errors=\"coerce\")\n",
    "    print(f\"- Rango temporal: {df_anom['date_parsed'].min()} a {df_anom['date_parsed'].max()}\")\n",
    "    print(f\"- Tweets por fuera de 2021: {len(df_anom[df_anom['date_parsed'].dt.year != 2021])}\")\n",
    "\n",
    "analyze_twitter_nuances(file_path)\n",
    "advanced_analysis(file_path)\n",
    "discover_anomalies(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definici√≥n de estaratefias de optimizaci√≥n\n",
    "\n",
    "Depues de analizar los datos y el c√≥digo,ver detalle en la secci√≥n 3 del archivo [SOLUTION.md](../SOLUTION.md) \n",
    "\n",
    "\n",
    "\n",
    "## Calidad de software\n",
    "\n",
    "Se proponen realizar test basados en los resultados del analisis de los datos. ver detalle en la secci√≥n 4 del archivo [SOLUTION.md](../SOLUTION.md. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C√≥digo\n",
    "\n",
    "El proceso de optimizaci√≥n de el procesamiento de datos empieza desde el proceso de lectura\n",
    "\n",
    "del archivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_standard_json(file_path: str) -> List[dict]:\n",
    "    \"\"\"Lee y devuelve una lista completa de diccionarios (Standard).\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_streaming_orjson(file_path: str) -> Iterable[dict]:\n",
    "    \"\"\"Devuelve un generador (Lazy) de diccionarios usando orjson.\"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                yield orjson.loads(line)\n",
    "            except orjson.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "\n",
    "def read_chunks_orjson(file_path: str, chunk_size: int = 5000) -> Iterable[List[dict]]:\n",
    "    \"\"\"Lee y entrega bloques de 5000 registros sin funciones extra.\"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            try:\n",
    "                chunk.append(orjson.loads(line))\n",
    "                if len(chunk) == chunk_size:\n",
    "                    yield chunk\n",
    "                    chunk = []\n",
    "            except orjson.JSONDecodeError:\n",
    "                continue\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "\n",
    "\n",
    "def read_full_mem_json(file_path: str) -> List[dict]:\n",
    "    \"\"\"Lee todo el archivo en memoria y devuelve la lista de diccionarios.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [json.loads(line) for line in lines if line.strip()]\n",
    "\n",
    "\n",
    "def run_benchmarks(file_path: str):\n",
    "    print(f\"üöÄ INICIANDO BENCHMARK INTEGRAL DESACOPLADO: {file_path}\\n\")\n",
    "\n",
    "    strategies = [\n",
    "        (\"Standard JSON\", read_standard_json),\n",
    "        (\"Streaming Orjson\", read_streaming_orjson),\n",
    "        (\"Chunks Orjson\", read_chunks_orjson),\n",
    "        (\n",
    "            \"Polars read_ndjson (Eager)\",\n",
    "            lambda f: pl.read_ndjson(f, infer_schema_length=None, ignore_errors=True),\n",
    "        ),\n",
    "        (\n",
    "            \"Polars scan_ndjson (Lazy)\",\n",
    "            lambda f: pl.scan_ndjson(f, infer_schema_length=None, ignore_errors=True),\n",
    "        ),\n",
    "        (\n",
    "            \"Polars read_ndjson (Eager) + schema\",\n",
    "            lambda f: pl.read_ndjson(f, schema=twitter_schema, ignore_errors=True),\n",
    "        ),\n",
    "        (\n",
    "            \"Polars scan_ndjson (Lazy) + schema\",\n",
    "            lambda f: pl.scan_ndjson(f, schema=twitter_schema, ignore_errors=True),\n",
    "        ),\n",
    "        (\"Full Memory (readlines)\", read_full_mem_json),\n",
    "        (\"Pandas read_json\", lambda f: pd.read_json(f, lines=True)),\n",
    "    ]\n",
    "\n",
    "    for name, func in strategies:\n",
    "        print(f\"\\n{'#' * 70}\")\n",
    "        print(f\"### ESTRATEGIA: {name}\")\n",
    "        print(f\"{'#' * 70}\")\n",
    "\n",
    "        # Medici√≥n 1: Rendimiento Real (Tiempo Wall-clock + Pico RAM)\n",
    "        # Esto es lo que realmente importa para la eficiencia base\n",
    "        perf_monitor = profile_performance(func)\n",
    "        perf_monitor(file_path)\n",
    "\n",
    "        # Medici√≥n 2: An√°lisis T√©cnico (Opcional - Reporte cProfile)\n",
    "        # Se ejecuta aparte para no influir en los tiempos reales de arriba\n",
    "        detailed_monitor = profile_detailed(func)\n",
    "        detailed_monitor(file_path)\n",
    "\n",
    "\n",
    "run_benchmarks(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de haber definido los tipos de lectura de archivos que se utilizar√°n para cada proceso se procede a experimentar con el rendimiento de las funcions que ayudaran en la soluci√≥n de las preguntas, para esto se definen funciones qeu permiten realizar las preubas m√°s facilmente, sin embargo debido  a latencia qeu produce la ejecuci√≥n en JUpiter notebooks se deja este analisis en el archivo [benchmark.py](../benchmark.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
