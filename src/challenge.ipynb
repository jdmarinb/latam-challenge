{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu soluci√≥n y todas las suposiciones que est√°s considerando. Aqu√≠ puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import emoji\n",
    "import random\n",
    "import orjson\n",
    "import unicodedata\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Iterable, Callable\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "\n",
    "\n",
    "# Local libraries\n",
    "from utils import twitter_schema\n",
    "from benchmark import profile_performance, profile_detailed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "El archivo [SOLUTION.md](SOLUTION.md) contiene la explicaci√≥n detallada del paso apaso de la soluci√≥n\n",
    "\n",
    "## Ambiente de desarrollo\n",
    "\n",
    "Se utiliza make para desarrollar el proyecto. Puedes ver las opciones disponibles en el archivo [Makefile](Makefile).\n",
    "\n",
    "## Data Analisis\n",
    "\n",
    "Se realiza un analisis de los datos para comprender su estructura y posibles problemas de calidad. ver detalle en la secci√≥n 2 del archivo [SOLUTION.md](../SOLUTION.md) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_twitter_nuances(file_path, sample_size=20000):\n",
    "    print(f\"--- üïµÔ∏è‚Äç‚ôÇÔ∏è Deep Dive Analysis: {file_path} ---\")\n",
    "\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= sample_size:\n",
    "                    break\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Archivo no encontrado.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # ==========================================\n",
    "    # 1. AN√ÅLISIS DE TRUNCAMIENTO (Extended Mode)\n",
    "    # ==========================================\n",
    "    print(\"\\n[1] AN√ÅLISIS DE TRUNCAMIENTO\")\n",
    "\n",
    "    # Verificar si existen claves nativas de la API v1.1\n",
    "    has_truncated_key = \"truncated\" in df.columns\n",
    "    has_extended_tweet = \"extended_tweet\" in df.columns\n",
    "\n",
    "    if has_truncated_key:\n",
    "        truncated_count = df[\"truncated\"].sum() if df[\"truncated\"].dtype == bool else 0\n",
    "        print(\n",
    "            f\" - Tweets marcados como 'truncated': {truncated_count} ({truncated_count / len(df):.2%})\"\n",
    "        )\n",
    "\n",
    "        if has_extended_tweet:\n",
    "            extended_count = df[\"extended_tweet\"].notnull().sum()\n",
    "            print(f\" - Tweets con objeto 'extended_tweet' disponible: {extended_count}\")\n",
    "    else:\n",
    "        print(\n",
    "            \" - La clave 'truncated' NO existe en este dataset (probablemente ya fue procesado/aplanado).\"\n",
    "        )\n",
    "\n",
    "    # Verificar visualmente si el contenido parece cortado\n",
    "    # Los tweets truncados suelen terminar en \"...\" o un enlace t.co\n",
    "    df[\"ends_with_ellipsis\"] = df[\"content\"].astype(str).str.strip().str.endswith(\"‚Ä¶\")\n",
    "    suspicious_truncation = df[\"ends_with_ellipsis\"].sum()\n",
    "    print(f\" - Tweets que terminan visualmente en '‚Ä¶': {suspicious_truncation}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. AN√ÅLISIS DE RETWEETS (Duplicidad)\n",
    "    # ==========================================\n",
    "    print(\"\\n[2] AN√ÅLISIS DE RETWEETS\")\n",
    "\n",
    "    # Detectar RTs\n",
    "    # Opci√≥n A: Clave 'retweeted_status' (Standard API)\n",
    "    if \"retweeted_status\" in df.columns:\n",
    "        rts_count = df[\"retweeted_status\"].notnull().sum()\n",
    "        print(f\" - Detectados por metadato 'retweeted_status': {rts_count}\")\n",
    "\n",
    "    # Opci√≥n B: Texto empieza con \"RT @\"\n",
    "    df[\"is_rt_text\"] = df[\"content\"].astype(str).str.startswith(\"RT @\")\n",
    "    rts_text_count = df[\"is_rt_text\"].sum()\n",
    "    print(\n",
    "        f\" - Detectados por texto ('RT @...'): {rts_text_count} ({rts_text_count / len(df):.2%})\"\n",
    "    )\n",
    "\n",
    "    if rts_text_count > 0:\n",
    "        print(\n",
    "            \"   -> CONCLUSI√ìN: Los Retweets est√°n presentes. Q2 y Q3 estar√°n inflados por repetici√≥n.\"\n",
    "        )\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. MENCIONES: TEXTO VS METADATA (Q3)\n",
    "    # ==========================================\n",
    "    print(\"\\n[3] COMPARATIVA DE MENCIONES (Q3)\")\n",
    "\n",
    "    # Funci√≥n para extraer menciones con Regex (enfoque ingenuo)\n",
    "    def extract_regex_mentions(text):\n",
    "        return set(re.findall(r\"@(\\w+)\", str(text)))\n",
    "\n",
    "    # Funci√≥n para extraer menciones de Metadata (enfoque robusto)\n",
    "    def extract_meta_mentions(mentions_list):\n",
    "        if not isinstance(mentions_list, list):\n",
    "            return set()\n",
    "        return set(\n",
    "            m.get(\"username\")\n",
    "            for m in mentions_list\n",
    "            if isinstance(m, dict) and m.get(\"username\")\n",
    "        )\n",
    "\n",
    "    # Aplicar comparativa en una muestra peque√±a para velocidad\n",
    "    sample_df = df.head(1000).copy()\n",
    "\n",
    "    sample_df[\"regex_mentions\"] = sample_df[\"content\"].apply(extract_regex_mentions)\n",
    "    sample_df[\"meta_mentions\"] = sample_df[\"mentionedUsers\"].apply(\n",
    "        extract_meta_mentions\n",
    "    )\n",
    "\n",
    "    # Buscar discrepancias\n",
    "    # Casos donde Metadata tiene ALGO pero Regex NO (Menciones invisibles/Reply)\n",
    "    sample_df[\"hidden_mentions\"] = sample_df.apply(\n",
    "        lambda x: x[\"meta_mentions\"] - x[\"regex_mentions\"], axis=1\n",
    "    )\n",
    "    hidden_count = sample_df[sample_df[\"hidden_mentions\"].astype(bool)].shape[0]\n",
    "\n",
    "    # Casos donde Regex tiene ALGO pero Metadata NO (Falsos positivos, emails, usuarios suspendidos)\n",
    "    sample_df[\"fake_mentions\"] = sample_df.apply(\n",
    "        lambda x: x[\"regex_mentions\"] - x[\"meta_mentions\"], axis=1\n",
    "    )\n",
    "    fake_count = sample_df[sample_df[\"fake_mentions\"].astype(bool)].shape[0]\n",
    "\n",
    "    print(f\"An√°lisis sobre {len(sample_df)} registros:\")\n",
    "    print(\n",
    "        f\" - Casos donde Metadata detecta usuarios que Regex NO ve (Hidden/Reply): {hidden_count}\"\n",
    "    )\n",
    "    if hidden_count > 0:\n",
    "        example = sample_df[sample_df[\"hidden_mentions\"].astype(bool)].iloc[0]\n",
    "        print(\n",
    "            f\"   Ejemplo Hidden -> Texto: '{example['content'][:50]}...' | Meta: {example['meta_mentions']}\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\" - Casos donde Regex detecta '@' que NO son usuarios v√°lidos en Metadata: {fake_count}\"\n",
    "    )\n",
    "    if fake_count > 0:\n",
    "        example = sample_df[sample_df[\"fake_mentions\"].astype(bool)].iloc[0]\n",
    "        print(\n",
    "            f\"   Ejemplo Falso Positivo -> Texto: '{example['content'][:50]}...' | Regex: {example['regex_mentions']}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def advanced_analysis(file_path):\n",
    "    print(f\"--- üî¨ An√°lisis Forense Avanzado: {file_path} ---\")\n",
    "\n",
    "    usernames_raw = []\n",
    "    usernames_normalized = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    tweet = json.loads(line)\n",
    "                    u = tweet.get(\"user\", {}).get(\"username\")\n",
    "                    if u:\n",
    "                        usernames_raw.append(u)\n",
    "                        # Normalizaci√≥n NFKC + Lowercase\n",
    "                        usernames_normalized.append(\n",
    "                            unicodedata.normalize(\"NFKC\", u).lower()\n",
    "                        )\n",
    "                except Exception:\n",
    "                    continue\n",
    "    except FileNotFoundError:\n",
    "        print(\"Archivo no encontrado\")\n",
    "        return\n",
    "\n",
    "    # 1. CHECK DE CASE SENSITIVITY Y UNICODE\n",
    "    unique_raw = len(set(usernames_raw))\n",
    "    unique_norm = len(set(usernames_normalized))\n",
    "\n",
    "    print(\"\\n[1] INTEGRIDAD DE ENTIDADES (Usernames)\")\n",
    "    print(f\" - Usuarios √∫nicos (Crudo): {unique_raw}\")\n",
    "    print(f\" - Usuarios √∫nicos (Normalizado + Lower): {unique_norm}\")\n",
    "    diff = unique_raw - unique_norm\n",
    "    if diff > 0:\n",
    "        print(\n",
    "            f\" ‚ö†Ô∏è ALERTA: Se detectaron {diff} duplicados por falta de normalizaci√≥n/may√∫sculas.\"\n",
    "        )\n",
    "        print(\" -> ACCI√ìN: Es MANDATORIO aplicar .lower() y unicodedata.\")\n",
    "    else:\n",
    "        print(\n",
    "            \" -> OK: No se detectaron colisiones, pero es buena pr√°ctica implementarlo.\"\n",
    "        )\n",
    "\n",
    "    # 2. DETECCI√ìN DE BOTS (OUTLIERS Q1)\n",
    "    print(\"\\n[2] DISTRIBUCI√ìN DE ACTIVIDAD (Q1)\")\n",
    "    counts = Counter(usernames_raw)\n",
    "    top_5 = counts.most_common(5)\n",
    "\n",
    "    df = pd.Series(list(counts.values()))\n",
    "    p99 = df.quantile(0.99)\n",
    "    max_tweets = df.max()\n",
    "\n",
    "    print(f\" - Top 5 Usuarios m√°s activos:\\n   {top_5}\")\n",
    "    print(f\" - El 99% de usuarios tiene menos de {p99:.0f} tweets.\")\n",
    "    print(f\" - El usuario #1 tiene {max_tweets} tweets.\")\n",
    "\n",
    "    if max_tweets > (p99 * 10):\n",
    "        print(\n",
    "            f\" ‚ö†Ô∏è ALERTA: El usuario top tiene una actividad {max_tweets / p99:.1f}x mayor al promedio.\"\n",
    "        )\n",
    "        print(\n",
    "            \" -> OBSERVACI√ìN: Probable Bot. Documentar en SOLUTION.md que esto sesga los resultados.\"\n",
    "        )\n",
    "\n",
    "    # 3. EMPATES (TIE-BREAKING)\n",
    "    print(\"\\n[3] RIESGO DE EMPATES EN EL CORTE\")\n",
    "    # Ver si hay muchos usuarios con el mismo conteo en el borde del top 10\n",
    "    counts_values = list(counts.values())\n",
    "    counts_freq = Counter(counts_values)\n",
    "\n",
    "    # Imaginemos que el corte del top 10 es alrededor de X tweets\n",
    "    sorted_counts = sorted(counts_values, reverse=True)\n",
    "    if len(sorted_counts) > 10:\n",
    "        val_at_10 = sorted_counts[9]  # El valor del d√©cimo lugar\n",
    "        users_at_cutoff = counts_freq[val_at_10]\n",
    "        print(f\" - Valor de corte (Puesto #10): {val_at_10} tweets\")\n",
    "        print(f\" - Cu√°ntos usuarios tienen exactamente ese valor: {users_at_cutoff}\")\n",
    "\n",
    "        if users_at_cutoff > 1:\n",
    "            print(\" ‚ö†Ô∏è ALERTA CR√çTICA: Hay EMPATE en el puesto #10.\")\n",
    "            print(\n",
    "                \" -> ACCI√ìN: Tu c√≥digo DEBE tener un criterio de desempate (ej: alfab√©tico) o los tests fallar√°n aleatoriamente.\"\n",
    "            )\n",
    "\n",
    "\n",
    "def discover_anomalies(file_path, sample_size=15000):\n",
    "    print(f\"--- Iniciando An√°lisis Profundo: {file_path} (n={sample_size}) ---\")\n",
    "\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i < sample_size:\n",
    "                    data.append(line)\n",
    "                else:\n",
    "                    r = random.randint(0, i)\n",
    "                    if r < sample_size:\n",
    "                        data[r] = line\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo {file_path} no existe.\")\n",
    "        return\n",
    "\n",
    "    parsed = []\n",
    "    corrupt_lines = 0\n",
    "    for line in data:\n",
    "        try:\n",
    "            parsed.append(json.loads(line))\n",
    "        except Exception:\n",
    "            corrupt_lines += 1\n",
    "\n",
    "    if corrupt_lines:\n",
    "        print(f\"[ALERTA] Se detectaron {corrupt_lines} l√≠neas corruptas en la muestra.\")\n",
    "\n",
    "    # Normalizaci√≥n para an√°lisis\n",
    "    df = pd.json_normalize(parsed)\n",
    "\n",
    "    print(\"\\n1. INTEGRIDAD DE COLUMNAS CLAVE\")\n",
    "    columns_to_check = [\"date\", \"content\", \"mentionedUsers\", \"user.username\", \"user.id\"]\n",
    "    for col in columns_to_check:\n",
    "        if col in df.columns:\n",
    "            types = df[col].apply(lambda x: type(x).__name__).value_counts()\n",
    "            nulls = df[col].isnull().sum()\n",
    "            print(f\"- '{col}': {len(types)} tipos detectados. Nulos: {nulls}\")\n",
    "        else:\n",
    "            print(f\"- [ERROR] Columna '{col}' NO encontrada.\")\n",
    "\n",
    "    print(\"\\n2. AN√ÅLISIS DE USUARIOS (Q1 & Q3)\")\n",
    "    # Verificar si un username tiene m√∫ltiples IDs (cambio de handle)\n",
    "    user_consistency = df.groupby(\"user.username\")[\"user.id\"].nunique()\n",
    "    inconsistent = user_consistency[user_consistency > 1]\n",
    "    print(f\"- Usuarios con m√°s de un ID: {len(inconsistent)}\")\n",
    "\n",
    "    # Calcular potencial de String Interning\n",
    "    total_names = len(df[\"user.username\"])\n",
    "    unique_names = df[\"user.username\"].nunique()\n",
    "    print(\n",
    "        f\"- Ratio de Repetici√≥n de Usernames: {total_names / unique_names:.2f}x (Alto ratio justifica sys.intern)\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n3. AN√ÅLISIS DE MENCIONES (Q3)\")\n",
    "    # Validar estructura interna de mentionedUsers\n",
    "    mentions_data = df[\"mentionedUsers\"].dropna()\n",
    "    has_nested_nulls = mentions_data.apply(\n",
    "        lambda x: any(m.get(\"username\") is None for m in x if isinstance(m, dict))\n",
    "    ).sum()\n",
    "    print(f\"- Registros con listas de menciones v√°lidas: {len(mentions_data)}\")\n",
    "    print(f\"- Listas con objetos internos nulos: {has_nested_nulls}\")\n",
    "\n",
    "    print(\"\\n4. AN√ÅLISIS DE EMOJIS COMPLEJOS (Q2)\")\n",
    "    all_emojis = []\n",
    "    complex_count = 0\n",
    "    for txt in df[\"content\"].dropna():\n",
    "        # emoji_list devuelve informaci√≥n detallada de cada emoji\n",
    "        found = emoji.emoji_list(txt)\n",
    "        for e in found:\n",
    "            char = e[\"emoji\"]\n",
    "            all_emojis.append(char)\n",
    "            # Si tiene m√°s de un componente unicode o caracteres especiales de uni√≥n\n",
    "            if len(char) > 1 or \"\\u200d\" in char:\n",
    "                complex_count += 1\n",
    "\n",
    "    print(f\"- Total emojis detectados: {len(all_emojis)}\")\n",
    "    print(f\"- Emojis complejos (ZWJ/Multi-char): {complex_count}\")\n",
    "    if all_emojis:\n",
    "        print(f\"- Top 3 Emojis en muestra: {Counter(all_emojis).most_common(3)}\")\n",
    "\n",
    "    print(\"\\n5. VALORES EXTREMOS Y FECHAS\")\n",
    "    df[\"date_parsed\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    print(f\"- Rango temporal: {df['date_parsed'].min()} a {df['date_parsed'].max()}\")\n",
    "    print(f\"- Tweets por fuera de 2021: {len(df[df['date_parsed'].dt.year != 2021])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üïµÔ∏è‚Äç‚ôÇÔ∏è Deep Dive Analysis: ../farmers-protest-tweets-2021-2-4.json ---\n",
      "\n",
      "[1] AN√ÅLISIS DE TRUNCAMIENTO\n",
      " - La clave 'truncated' NO existe en este dataset (probablemente ya fue procesado/aplanado).\n",
      " - Tweets que terminan visualmente en '‚Ä¶': 4\n",
      "\n",
      "[2] AN√ÅLISIS DE RETWEETS\n",
      " - Detectados por texto ('RT @...'): 10 (0.05%)\n",
      "   -> CONCLUSI√ìN: Los Retweets est√°n presentes. Q2 y Q3 estar√°n inflados por repetici√≥n.\n",
      "\n",
      "[3] COMPARATIVA DE MENCIONES (Q3)\n",
      "An√°lisis sobre 1000 registros:\n",
      " - Casos donde Metadata detecta usuarios que Regex NO ve (Hidden/Reply): 14\n",
      "   Ejemplo Hidden -> Texto: '.@RakeshTikaitBKU ‡§¨‡•ã‡§≤‡•á- ‡§∏‡§Ç‡§∏‡§¶ ‡§ú‡§æ‡§ï‡§∞ ‡§ü‡•ç‡§∞‡•à‡§ï‡•ç‡§ü‡§∞ ‡§ö‡§≤‡§æ‡§è‡§Ç‡§ó‡•á...' | Meta: {'RakeshTikaitBKU', 'KumarKunalmedia'}\n",
      " - Casos donde Regex detecta '@' que NO son usuarios v√°lidos en Metadata: 15\n",
      "   Ejemplo Falso Positivo -> Texto: '.@RakeshTikaitBKU ‡§¨‡•ã‡§≤‡•á- ‡§∏‡§Ç‡§∏‡§¶ ‡§ú‡§æ‡§ï‡§∞ ‡§ü‡•ç‡§∞‡•à‡§ï‡•ç‡§ü‡§∞ ‡§ö‡§≤‡§æ‡§è‡§Ç‡§ó‡•á...' | Regex: {'RakeshTikaitBKU', 'kumarkunalmedia'}\n"
     ]
    }
   ],
   "source": [
    "analyze_twitter_nuances(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üî¨ An√°lisis Forense Avanzado: ../farmers-protest-tweets-2021-2-4.json ---\n",
      "\n",
      "[1] INTEGRIDAD DE ENTIDADES (Usernames)\n",
      " - Usuarios √∫nicos (Crudo): 26519\n",
      " - Usuarios √∫nicos (Normalizado + Lower): 26519\n",
      " -> OK: No se detectaron colisiones, pero es buena pr√°ctica implementarlo.\n",
      "\n",
      "[2] DISTRIBUCI√ìN DE ACTIVIDAD (Q1)\n",
      " - Top 5 Usuarios m√°s activos:\n",
      "   [('jot__b', 1019), ('rebelpacifist', 850), ('MaanDee08215437', 830), ('Gurpreetd86', 636), ('GurmVicky', 597)]\n",
      " - El 99% de usuarios tiene menos de 56 tweets.\n",
      " - El usuario #1 tiene 1019 tweets.\n",
      " ‚ö†Ô∏è ALERTA: El usuario top tiene una actividad 18.2x mayor al promedio.\n",
      " -> OBSERVACI√ìN: Probable Bot. Documentar en SOLUTION.md que esto sesga los resultados.\n",
      "\n",
      "[3] RIESGO DE EMPATES EN EL CORTE\n",
      " - Valor de corte (Puesto #10): 490 tweets\n",
      " - Cu√°ntos usuarios tienen exactamente ese valor: 1\n"
     ]
    }
   ],
   "source": [
    "advanced_analysis(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando An√°lisis Profundo: ../farmers-protest-tweets-2021-2-4.json (n=15000) ---\n",
      "\n",
      "1. INTEGRIDAD DE COLUMNAS CLAVE\n",
      "- 'date': 1 tipos detectados. Nulos: 0\n",
      "- 'content': 1 tipos detectados. Nulos: 0\n",
      "- 'mentionedUsers': 2 tipos detectados. Nulos: 10107\n",
      "- 'user.username': 1 tipos detectados. Nulos: 0\n",
      "- 'user.id': 1 tipos detectados. Nulos: 0\n",
      "\n",
      "2. AN√ÅLISIS DE USUARIOS (Q1 & Q3)\n",
      "- Usuarios con m√°s de un ID: 0\n",
      "- Ratio de Repetici√≥n de Usernames: 2.18x (Alto ratio justifica sys.intern)\n",
      "\n",
      "3. AN√ÅLISIS DE MENCIONES (Q3)\n",
      "- Registros con listas de menciones v√°lidas: 4893\n",
      "- Listas con objetos internos nulos: 0\n",
      "\n",
      "4. AN√ÅLISIS DE EMOJIS COMPLEJOS (Q2)\n",
      "- Total emojis detectados: 5685\n",
      "- Emojis complejos (ZWJ/Multi-char): 1320\n",
      "- Top 3 Emojis en muestra: [('üôè', 663), ('üöú', 400), ('üòÇ', 386)]\n",
      "\n",
      "5. VALORES EXTREMOS Y FECHAS\n",
      "- Rango temporal: 2021-02-12 01:37:13+00:00 a 2021-02-24 09:22:35+00:00\n",
      "- Tweets por fuera de 2021: 0\n"
     ]
    }
   ],
   "source": [
    "discover_anomalies(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definici√≥n de estaratefias de optimizaci√≥n\n",
    "\n",
    "Depues de analizar los datos y el c√≥digo,ver detalle en la secci√≥n 3 del archivo [SOLUTION.md](../SOLUTION.md) \n",
    "\n",
    "\n",
    "## Calidad de software\n",
    "\n",
    "Se proponen realizar test basados en los resultados del analisis de los datos. ver detalle en la secci√≥n 4 del archivo [SOLUTION.md](../SOLUTION.md.\n",
    "\n",
    "\n",
    "## C√≥digo\n",
    "\n",
    "El proceso de optimizaci√≥n de el procesamiento de datos empieza desde el proceso de lectura\n",
    "del archivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ INICIANDO BENCHMARK INTEGRAL DESACOPLADO: ../farmers-protest-tweets-2021-2-4.json\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Standard JSON\n",
      "######################################################################\n",
      "\n",
      "[PERF] read_standard_json:\n",
      "  > Tiempo: 9.8950 s\n",
      "  > Memoria: 4500.62 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: read_standard_json ---\n",
      "         1391837 function calls (1391820 primitive calls) in 10.344 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 154 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        7    0.000    0.000   11.810    1.687 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:319(select)\n",
      "        7    0.000    0.000   11.810    1.687 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:313(_select)\n",
      "   117407    0.165    0.000    7.203    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\__init__.py:299(loads)\n",
      "   117407    0.400    0.000    6.966    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\decoder.py:333(decode)\n",
      "   117407    6.286    0.000    6.286    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\decoder.py:344(raw_decode)\n",
      "        1    0.373    0.373    1.874    1.874 C:\\Users\\johanmarin\\AppData\\Local\\Temp\\ipykernel_17536\\2052901737.py:5(read_standard_json)\n",
      "        6    1.507    0.251    1.668    0.278 {built-in method select.select}\n",
      "   234814    0.182    0.000    0.182    0.000 {method 'match' of 're.Pattern' objects}\n",
      "    49772    0.060    0.000    0.141    0.000 <frozen codecs>:319(decode)\n",
      "    49772    0.081    0.000    0.081    0.000 {built-in method _codecs.utf_8_decode}\n",
      "   234814    0.067    0.000    0.067    0.000 {method 'end' of 're.Match' objects}\n",
      "   117407    0.048    0.000    0.048    0.000 {method 'startswith' of 'str' objects}\n",
      "   117412    0.035    0.000    0.035    0.000 {method 'append' of 'list' objects}\n",
      "   117433    0.031    0.000    0.031    0.000 {built-in method builtins.len}\n",
      "117564/117558    0.025    0.000    0.025    0.000 {built-in method builtins.isinstance}\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: read_standard_json ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Streaming Orjson\n",
      "######################################################################\n",
      "\n",
      "[PERF] read_streaming_orjson:\n",
      "  > Tiempo: 0.0000 s\n",
      "  > Memoria: 3776.40 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: read_streaming_orjson ---\n",
      "         1 function calls in 0.000 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: read_streaming_orjson ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Chunks Orjson\n",
      "######################################################################\n",
      "\n",
      "[PERF] read_chunks_orjson:\n",
      "  > Tiempo: 0.0000 s\n",
      "  > Memoria: 3776.40 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: read_chunks_orjson ---\n",
      "         1 function calls in 0.000 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: read_chunks_orjson ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Polars read_ndjson (Eager)\n",
      "######################################################################\n",
      "\n",
      "[PERF] <lambda>:\n",
      "  > Tiempo: 6.6494 s\n",
      "  > Memoria: 5121.51 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: <lambda> ---\n",
      "         515 function calls (511 primitive calls) in 6.974 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 170 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      2/1    0.000    0.000    6.973    6.973 C:\\Users\\johanmarin\\AppData\\Local\\Temp\\ipykernel_17536\\2052901737.py:55(<lambda>)\n",
      "        1    0.078    0.078    6.973    6.973 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\io\\ndjson.py:25(read_ndjson)\n",
      "        3    0.000    0.000    4.699    1.566 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\asyncio\\base_events.py:1922(_run_once)\n",
      "        3    0.000    0.000    4.698    1.566 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:319(select)\n",
      "        3    0.000    0.000    4.698    1.566 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:313(_select)\n",
      "        3    4.698    1.566    4.698    1.566 {built-in method select.select}\n",
      "        1    0.000    0.000    2.195    2.195 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:84(wrapper)\n",
      "        1    0.000    0.000    2.195    2.195 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:308(wrapper)\n",
      "        1    0.000    0.000    2.195    2.195 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2198(collect)\n",
      "        1    2.195    2.195    2.195    2.195 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "        4    0.000    0.000    0.001    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\asyncio\\events.py:86(_run)\n",
      "        4    0.000    0.000    0.001    0.000 {method 'run' of '_contextvars.Context' objects}\n",
      "        1    0.000    0.000    0.001    0.001 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\tornado\\ioloop.py:750(_run_callback)\n",
      "        1    0.000    0.000    0.001    0.001 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\ipykernel\\iostream.py:611(_flush)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py:206(_handle_events)\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: <lambda> ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Polars scan_ndjson (Lazy)\n",
      "######################################################################\n",
      "\n",
      "[PERF] <lambda>:\n",
      "  > Tiempo: 0.0001 s\n",
      "  > Memoria: 4763.54 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: <lambda> ---\n",
      "         33 function calls (32 primitive calls) in 0.000 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 27 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\johanmarin\\AppData\\Local\\Temp\\ipykernel_17536\\2052901737.py:56(<lambda>)\n",
      "      2/1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:123(wrapper)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\io\\ndjson.py:177(scan_ndjson)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\io\\cloud\\credential_provider\\_builder.py:303(_init_credential_provider_builder)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method new_from_ndjson}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\logging.py:7(verbose)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen os>:808(getenv)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen _collections_abc>:804(get)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\various.py:232(normalize_filepath)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\io\\cloud\\credential_provider\\_builder.py:312(f)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen os>:709(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\wrap.py:16(wrap_ldf)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen ntpath>:351(expanduser)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\lazyframe\\frame.py:429(_from_pyldf)\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: <lambda> ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Polars read_ndjson (Eager) + schema\n",
      "######################################################################\n",
      "\n",
      "[PERF] <lambda>:\n",
      "  > Tiempo: 0.3269 s\n",
      "  > Memoria: 4761.54 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: <lambda> ---\n",
      "         577 function calls (571 primitive calls) in 0.323 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 163 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      2/1    0.000    0.000    0.322    0.322 C:\\Users\\johanmarin\\AppData\\Local\\Temp\\ipykernel_17536\\2052901737.py:57(<lambda>)\n",
      "        1    0.001    0.001    0.322    0.322 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\io\\ndjson.py:25(read_ndjson)\n",
      "        2    0.000    0.000    0.207    0.104 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\asyncio\\base_events.py:1922(_run_once)\n",
      "        2    0.000    0.000    0.205    0.103 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:319(select)\n",
      "        2    0.000    0.000    0.205    0.103 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:313(_select)\n",
      "        2    0.205    0.103    0.205    0.103 {built-in method select.select}\n",
      "        1    0.000    0.000    0.114    0.114 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:84(wrapper)\n",
      "        1    0.000    0.000    0.114    0.114 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:308(wrapper)\n",
      "        1    0.000    0.000    0.114    0.114 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2198(collect)\n",
      "        1    0.114    0.114    0.114    0.114 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "        3    0.000    0.000    0.002    0.001 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\asyncio\\events.py:86(_run)\n",
      "        3    0.000    0.000    0.002    0.001 {method 'run' of '_contextvars.Context' objects}\n",
      "        2    0.000    0.000    0.001    0.001 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py:206(_handle_events)\n",
      "        2    0.000    0.000    0.001    0.001 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py:573(_handle_events)\n",
      "        1    0.000    0.000    0.001    0.001 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\tornado\\ioloop.py:750(_run_callback)\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: <lambda> ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Polars scan_ndjson (Lazy) + schema\n",
      "######################################################################\n",
      "\n",
      "[PERF] <lambda>:\n",
      "  > Tiempo: 0.0001 s\n",
      "  > Memoria: 3138.62 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: <lambda> ---\n",
      "         33 function calls (32 primitive calls) in 0.000 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 27 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\johanmarin\\AppData\\Local\\Temp\\ipykernel_17536\\2052901737.py:58(<lambda>)\n",
      "      2/1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:123(wrapper)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\io\\ndjson.py:177(scan_ndjson)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\io\\cloud\\credential_provider\\_builder.py:303(_init_credential_provider_builder)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method new_from_ndjson}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\logging.py:7(verbose)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen os>:808(getenv)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen _collections_abc>:804(get)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\various.py:232(normalize_filepath)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\io\\cloud\\credential_provider\\_builder.py:312(f)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen os>:709(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\wrap.py:16(wrap_ldf)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen ntpath>:351(expanduser)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen os>:783(encodekey)\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: <lambda> ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Full Memory (readlines)\n",
      "######################################################################\n",
      "\n",
      "[PERF] read_full_mem_json:\n",
      "  > Tiempo: 11.0624 s\n",
      "  > Memoria: 4257.37 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: read_full_mem_json ---\n",
      "         1391838 function calls (1391826 primitive calls) in 10.535 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 156 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        7    0.079    0.011   12.111    1.730 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:319(select)\n",
      "   117407    0.149    0.000    7.095    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\__init__.py:299(loads)\n",
      "   117407    1.632    0.000    6.881    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\decoder.py:333(decode)\n",
      "   117407    4.988    0.000    4.988    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\decoder.py:344(raw_decode)\n",
      "        6    0.000    0.000    1.789    0.298 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:313(_select)\n",
      "        6    1.771    0.295    1.786    0.298 {built-in method select.select}\n",
      "        1    1.185    1.185    1.278    1.278 {method 'readlines' of '_io._IOBase' objects}\n",
      "        1    0.025    0.025    1.059    1.059 C:\\Users\\johanmarin\\AppData\\Local\\Temp\\ipykernel_17536\\2052901737.py:41(read_full_mem_json)\n",
      "   234814    0.166    0.000    0.166    0.000 {method 'match' of 're.Pattern' objects}\n",
      "   117407    0.151    0.000    0.151    0.000 {method 'strip' of 'str' objects}\n",
      "    49772    0.033    0.000    0.108    0.000 <frozen codecs>:319(decode)\n",
      "    49772    0.075    0.000    0.075    0.000 {built-in method _codecs.utf_8_decode}\n",
      "   234814    0.066    0.000    0.066    0.000 {method 'end' of 're.Match' objects}\n",
      "   117407    0.043    0.000    0.043    0.000 {method 'startswith' of 'str' objects}\n",
      "   117433    0.029    0.000    0.029    0.000 {built-in method builtins.len}\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: read_full_mem_json ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Pandas read_json\n",
      "######################################################################\n",
      "\n",
      "[PERF] <lambda>:\n",
      "  > Tiempo: 10.4132 s\n",
      "  > Memoria: 5492.58 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: <lambda> ---\n",
      "         486596 function calls (486236 primitive calls) in 10.267 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 550 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.024    0.024    8.832    8.832 C:\\Users\\johanmarin\\AppData\\Local\\Temp\\ipykernel_17536\\2052901737.py:60(<lambda>)\n",
      "        1    0.063    0.063    8.809    8.809 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:505(read_json)\n",
      "        6    0.000    0.000    6.105    1.018 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:319(select)\n",
      "        6    0.180    0.030    6.105    1.018 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:313(_select)\n",
      "        6    0.723    0.121    5.914    0.986 {built-in method select.select}\n",
      "        1    5.062    5.062    5.062    5.062 {built-in method pandas._libs.json.ujson_loads}\n",
      "        1    0.022    0.022    2.099    2.099 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:991(read)\n",
      "        1    0.000    0.000    2.077    2.077 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1022(_get_object_parser)\n",
      "        1    0.000    0.000    2.077    2.077 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1174(parse)\n",
      "        3    0.000    0.000    1.467    0.489 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:698(__init__)\n",
      "        6    0.000    0.000    1.434    0.239 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\asyncio\\events.py:86(_run)\n",
      "        6    0.000    0.000    1.434    0.239 {method 'run' of '_contextvars.Context' objects}\n",
      "        1    0.206    0.206    1.356    1.356 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1386(_parse)\n",
      "       15    0.000    0.000    1.128    0.075 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\zmq\\sugar\\socket.py:623(send)\n",
      "        1    0.000    0.000    0.942    0.942 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:506(nested_data_to_arrays)\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: <lambda> ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_standard_json(file_path: str) -> List[dict]:\n",
    "    \"\"\"Lee y devuelve una lista completa de diccionarios (Standard).\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_streaming_orjson(file_path: str) -> Iterable[dict]:\n",
    "    \"\"\"Devuelve un generador (Lazy) de diccionarios usando orjson.\"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                yield orjson.loads(line)\n",
    "            except orjson.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "\n",
    "def read_chunks_orjson(file_path: str, chunk_size: int = 5000) -> Iterable[List[dict]]:\n",
    "    \"\"\"Lee y entrega bloques de 5000 registros sin funciones extra.\"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            try:\n",
    "                chunk.append(orjson.loads(line))\n",
    "                if len(chunk) == chunk_size:\n",
    "                    yield chunk\n",
    "                    chunk = []\n",
    "            except orjson.JSONDecodeError:\n",
    "                continue\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "\n",
    "\n",
    "def read_full_mem_json(file_path: str) -> List[dict]:\n",
    "    \"\"\"Lee todo el archivo en memoria y devuelve la lista de diccionarios.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [json.loads(line) for line in lines if line.strip()]\n",
    "\n",
    "\n",
    "def run_benchmarks(file_path: str):\n",
    "    print(f\"üöÄ INICIANDO BENCHMARK INTEGRAL DESACOPLADO: {file_path}\\n\")\n",
    "\n",
    "    strategies = [\n",
    "        (\"Standard JSON\", read_standard_json),\n",
    "        (\"Streaming Orjson\", read_streaming_orjson),\n",
    "        (\"Chunks Orjson\", read_chunks_orjson),\n",
    "        (\n",
    "            \"Polars read_ndjson (Eager)\",\n",
    "            lambda f: pl.read_ndjson(f, infer_schema_length=None, ignore_errors=True),\n",
    "        ),\n",
    "        (\n",
    "            \"Polars scan_ndjson (Lazy)\",\n",
    "            lambda f: pl.scan_ndjson(f, infer_schema_length=None, ignore_errors=True),\n",
    "        ),\n",
    "        (\n",
    "            \"Polars read_ndjson (Eager) + schema\",\n",
    "            lambda f: pl.read_ndjson(f, schema=twitter_schema, ignore_errors=True),\n",
    "        ),\n",
    "        (\n",
    "            \"Polars scan_ndjson (Lazy) + schema\",\n",
    "            lambda f: pl.scan_ndjson(f, schema=twitter_schema, ignore_errors=True),\n",
    "        ),\n",
    "        (\"Full Memory (readlines)\", read_full_mem_json),\n",
    "        (\"Pandas read_json\", lambda f: pd.read_json(f, lines=True)),\n",
    "    ]\n",
    "\n",
    "    for name, func in strategies:\n",
    "        print(f\"\\n{'#' * 70}\")\n",
    "        print(f\"### ESTRATEGIA: {name}\")\n",
    "        print(f\"{'#' * 70}\")\n",
    "\n",
    "        # Medici√≥n 1: Rendimiento Real (Tiempo Wall-clock + Pico RAM)\n",
    "        # Esto es lo que realmente importa para la eficiencia base\n",
    "        perf_monitor = profile_performance(func)\n",
    "        perf_monitor(file_path)\n",
    "\n",
    "        # Medici√≥n 2: An√°lisis T√©cnico (Opcional - Reporte cProfile)\n",
    "        # Se ejecuta aparte para no influir en los tiempos reales de arriba\n",
    "        detailed_monitor = profile_detailed(func)\n",
    "        detailed_monitor(file_path)\n",
    "\n",
    "\n",
    "run_benchmarks(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de haber definido los tipos de lectura de archivos que se utilizar√°n para cada proceso se procede a experimentar con el rendimiento de las funcions que ayudaran en la soluci√≥n de las preguntas, para esto se definen funciones qeu permiten realizar las preubas m√°s facilemnte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_chunk_reader(file_path: str, chunk_size: int = 5000) -> Iterable[List[bytes]]:\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            chunk.append(line)\n",
    "            if len(chunk) >= chunk_size:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "\n",
    "\n",
    "@profile_performance\n",
    "def lab_modular_test(\n",
    "    name: str, reader_func: Callable, processor_func: Callable, path: str\n",
    "):\n",
    "    print(f\"\\n[LAB] Escenario: {name}\")\n",
    "    data = reader_func(path)\n",
    "    return processor_func(data)\n",
    "\n",
    "\n",
    "@profile_performance\n",
    "def lab_parallel_test(name: str, file_path: str, mode: str = \"process\"):\n",
    "    print(f\"\\n[LAB] Escenario: {name} ({mode.upper()})\")\n",
    "    total_counts = Counter()\n",
    "    Executor = ProcessPoolExecutor if mode == \"process\" else ThreadPoolExecutor\n",
    "    with Executor() as executor:\n",
    "        results = executor.map(process_parallel_worker, text_chunk_reader(file_path))\n",
    "        for local_counts in results:\n",
    "            total_counts.update(local_counts)\n",
    "    return len(total_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dspues de definir los decoradores se preocede a realizar el benchmark para el codigo correspondiente a acad auna de las preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
