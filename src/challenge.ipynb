{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu soluci\u00f3n y todas las suposiciones que est\u00e1s considerando. Aqu\u00ed puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "El archivo [SOLUTION.md](SOLUTION.md) contiene la explicaci\u00f3n detallada del paso apaso de la soluci\u00f3n\n",
    "\n",
    "## Ambiente de desarrollo\n",
    "\n",
    "Se utiliza make para desarrollar el proyecto. Puedes ver las opciones disponibles en el archivo [Makefile](Makefile).\n",
    "\n",
    "## Data Analisis\n",
    "\n",
    "Se realiza un analisis de los datos para comprender su estructura y posibles problemas de calidad. ver detalle en la secci\u00f3n 2 del archivo [SOLUTION.md](SOLUTION.md) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "import emoji\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def analyze_twitter_nuances(file_path, sample_size=20000):\n",
    "    print(f\"--- \ud83d\udd75\ufe0f\u200d\u2642\ufe0f Deep Dive Analysis: {file_path} ---\")\n",
    "\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= sample_size:\n",
    "                    break\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Archivo no encontrado.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # ==========================================\n",
    "    # 1. AN\u00c1LISIS DE TRUNCAMIENTO (Extended Mode)\n",
    "    # ==========================================\n",
    "    print(\"\\n[1] AN\u00c1LISIS DE TRUNCAMIENTO\")\n",
    "\n",
    "    # Verificar si existen claves nativas de la API v1.1\n",
    "    has_truncated_key = \"truncated\" in df.columns\n",
    "    has_extended_tweet = \"extended_tweet\" in df.columns\n",
    "\n",
    "    if has_truncated_key:\n",
    "        truncated_count = df[\"truncated\"].sum() if df[\"truncated\"].dtype == bool else 0\n",
    "        print(\n",
    "            f\" - Tweets marcados como 'truncated': {truncated_count} ({truncated_count / len(df):.2%})\"\n",
    "        )\n",
    "\n",
    "        if has_extended_tweet:\n",
    "            extended_count = df[\"extended_tweet\"].notnull().sum()\n",
    "            print(f\" - Tweets con objeto 'extended_tweet' disponible: {extended_count}\")\n",
    "    else:\n",
    "        print(\n",
    "            \" - La clave 'truncated' NO existe en este dataset (probablemente ya fue procesado/aplanado).\"\n",
    "        )\n",
    "\n",
    "    # Verificar visualmente si el contenido parece cortado\n",
    "    # Los tweets truncados suelen terminar en \"...\" o un enlace t.co\n",
    "    df[\"ends_with_ellipsis\"] = df[\"content\"].astype(str).str.strip().str.endswith(\"\u2026\")\n",
    "    suspicious_truncation = df[\"ends_with_ellipsis\"].sum()\n",
    "    print(f\" - Tweets que terminan visualmente en '\u2026': {suspicious_truncation}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. AN\u00c1LISIS DE RETWEETS (Duplicidad)\n",
    "    # ==========================================\n",
    "    print(\"\\n[2] AN\u00c1LISIS DE RETWEETS\")\n",
    "\n",
    "    # Detectar RTs\n",
    "    # Opci\u00f3n A: Clave 'retweeted_status' (Standard API)\n",
    "    if \"retweeted_status\" in df.columns:\n",
    "        rts_count = df[\"retweeted_status\"].notnull().sum()\n",
    "        print(f\" - Detectados por metadato 'retweeted_status': {rts_count}\")\n",
    "\n",
    "    # Opci\u00f3n B: Texto empieza con \"RT @\"\n",
    "    df[\"is_rt_text\"] = df[\"content\"].astype(str).str.startswith(\"RT @\")\n",
    "    rts_text_count = df[\"is_rt_text\"].sum()\n",
    "    print(\n",
    "        f\" - Detectados por texto ('RT @...'): {rts_text_count} ({rts_text_count / len(df):.2%})\"\n",
    "    )\n",
    "\n",
    "    if rts_text_count > 0:\n",
    "        print(\n",
    "            \"   -> CONCLUSI\u00d3N: Los Retweets est\u00e1n presentes. Q2 y Q3 estar\u00e1n inflados por repetici\u00f3n.\"\n",
    "        )\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. MENCIONES: TEXTO VS METADATA (Q3)\n",
    "    # ==========================================\n",
    "    print(\"\\n[3] COMPARATIVA DE MENCIONES (Q3)\")\n",
    "\n",
    "    # Funci\u00f3n para extraer menciones con Regex (enfoque ingenuo)\n",
    "    def extract_regex_mentions(text):\n",
    "        return set(re.findall(r\"@(\\w+)\", str(text)))\n",
    "\n",
    "    # Funci\u00f3n para extraer menciones de Metadata (enfoque robusto)\n",
    "    def extract_meta_mentions(mentions_list):\n",
    "        if not isinstance(mentions_list, list):\n",
    "            return set()\n",
    "        return set(\n",
    "            m.get(\"username\")\n",
    "            for m in mentions_list\n",
    "            if isinstance(m, dict) and m.get(\"username\")\n",
    "        )\n",
    "\n",
    "    # Aplicar comparativa en una muestra peque\u00f1a para velocidad\n",
    "    sample_df = df.head(1000).copy()\n",
    "\n",
    "    sample_df[\"regex_mentions\"] = sample_df[\"content\"].apply(extract_regex_mentions)\n",
    "    sample_df[\"meta_mentions\"] = sample_df[\"mentionedUsers\"].apply(\n",
    "        extract_meta_mentions\n",
    "    )\n",
    "\n",
    "    # Buscar discrepancias\n",
    "    # Casos donde Metadata tiene ALGO pero Regex NO (Menciones invisibles/Reply)\n",
    "    sample_df[\"hidden_mentions\"] = sample_df.apply(\n",
    "        lambda x: x[\"meta_mentions\"] - x[\"regex_mentions\"], axis=1\n",
    "    )\n",
    "    hidden_count = sample_df[sample_df[\"hidden_mentions\"].astype(bool)].shape[0]\n",
    "\n",
    "    # Casos donde Regex tiene ALGO pero Metadata NO (Falsos positivos, emails, usuarios suspendidos)\n",
    "    sample_df[\"fake_mentions\"] = sample_df.apply(\n",
    "        lambda x: x[\"regex_mentions\"] - x[\"meta_mentions\"], axis=1\n",
    "    )\n",
    "    fake_count = sample_df[sample_df[\"fake_mentions\"].astype(bool)].shape[0]\n",
    "\n",
    "    print(f\"An\u00e1lisis sobre {len(sample_df)} registros:\")\n",
    "    print(\n",
    "        f\" - Casos donde Metadata detecta usuarios que Regex NO ve (Hidden/Reply): {hidden_count}\"\n",
    "    )\n",
    "    if hidden_count > 0:\n",
    "        example = sample_df[sample_df[\"hidden_mentions\"].astype(bool)].iloc[0]\n",
    "        print(\n",
    "            f\"   Ejemplo Hidden -> Texto: '{example['content'][:50]}...' | Meta: {example['meta_mentions']}\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\" - Casos donde Regex detecta '@' que NO son usuarios v\u00e1lidos en Metadata: {fake_count}\"\n",
    "    )\n",
    "    if fake_count > 0:\n",
    "        example = sample_df[sample_df[\"fake_mentions\"].astype(bool)].iloc[0]\n",
    "        print(\n",
    "            f\"   Ejemplo Falso Positivo -> Texto: '{example['content'][:50]}...' | Regex: {example['regex_mentions']}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def advanced_analysis(file_path):\n",
    "    print(f\"--- \ud83d\udd2c An\u00e1lisis Forense Avanzado: {file_path} ---\")\n",
    "\n",
    "    usernames_raw = []\n",
    "    usernames_normalized = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    tweet = json.loads(line)\n",
    "                    u = tweet.get(\"user\", {}).get(\"username\")\n",
    "                    if u:\n",
    "                        usernames_raw.append(u)\n",
    "                        # Normalizaci\u00f3n NFKC + Lowercase\n",
    "                        usernames_normalized.append(\n",
    "                            unicodedata.normalize(\"NFKC\", u).lower()\n",
    "                        )\n",
    "                except Exception:\n",
    "                    continue\n",
    "    except FileNotFoundError:\n",
    "        print(\"Archivo no encontrado\")\n",
    "        return\n",
    "\n",
    "    # 1. CHECK DE CASE SENSITIVITY Y UNICODE\n",
    "    unique_raw = len(set(usernames_raw))\n",
    "    unique_norm = len(set(usernames_normalized))\n",
    "\n",
    "    print(\"\\n[1] INTEGRIDAD DE ENTIDADES (Usernames)\")\n",
    "    print(f\" - Usuarios \u00fanicos (Crudo): {unique_raw}\")\n",
    "    print(f\" - Usuarios \u00fanicos (Normalizado + Lower): {unique_norm}\")\n",
    "    diff = unique_raw - unique_norm\n",
    "    if diff > 0:\n",
    "        print(\n",
    "            f\" \u26a0\ufe0f ALERTA: Se detectaron {diff} duplicados por falta de normalizaci\u00f3n/may\u00fasculas.\"\n",
    "        )\n",
    "        print(\" -> ACCI\u00d3N: Es MANDATORIO aplicar .lower() y unicodedata.\")\n",
    "    else:\n",
    "        print(\n",
    "            \" -> OK: No se detectaron colisiones, pero es buena pr\u00e1ctica implementarlo.\"\n",
    "        )\n",
    "\n",
    "    # 2. DETECCI\u00d3N DE BOTS (OUTLIERS Q1)\n",
    "    print(\"\\n[2] DISTRIBUCI\u00d3N DE ACTIVIDAD (Q1)\")\n",
    "    counts = Counter(usernames_raw)\n",
    "    top_5 = counts.most_common(5)\n",
    "\n",
    "    df = pd.Series(list(counts.values()))\n",
    "    p99 = df.quantile(0.99)\n",
    "    max_tweets = df.max()\n",
    "\n",
    "    print(f\" - Top 5 Usuarios m\u00e1s activos:\\n   {top_5}\")\n",
    "    print(f\" - El 99% de usuarios tiene menos de {p99:.0f} tweets.\")\n",
    "    print(f\" - El usuario #1 tiene {max_tweets} tweets.\")\n",
    "\n",
    "    if max_tweets > (p99 * 10):\n",
    "        print(\n",
    "            f\" \u26a0\ufe0f ALERTA: El usuario top tiene una actividad {max_tweets / p99:.1f}x mayor al promedio.\"\n",
    "        )\n",
    "        print(\n",
    "            \" -> OBSERVACI\u00d3N: Probable Bot. Documentar en SOLUTION.md que esto sesga los resultados.\"\n",
    "        )\n",
    "\n",
    "    # 3. EMPATES (TIE-BREAKING)\n",
    "    print(\"\\n[3] RIESGO DE EMPATES EN EL CORTE\")\n",
    "    # Ver si hay muchos usuarios con el mismo conteo en el borde del top 10\n",
    "    counts_values = list(counts.values())\n",
    "    counts_freq = Counter(counts_values)\n",
    "\n",
    "    # Imaginemos que el corte del top 10 es alrededor de X tweets\n",
    "    sorted_counts = sorted(counts_values, reverse=True)\n",
    "    if len(sorted_counts) > 10:\n",
    "        val_at_10 = sorted_counts[9]  # El valor del d\u00e9cimo lugar\n",
    "        users_at_cutoff = counts_freq[val_at_10]\n",
    "        print(f\" - Valor de corte (Puesto #10): {val_at_10} tweets\")\n",
    "        print(f\" - Cu\u00e1ntos usuarios tienen exactamente ese valor: {users_at_cutoff}\")\n",
    "\n",
    "        if users_at_cutoff > 1:\n",
    "            print(\" \u26a0\ufe0f ALERTA CR\u00cdTICA: Hay EMPATE en el puesto #10.\")\n",
    "            print(\n",
    "                \" -> ACCI\u00d3N: Tu c\u00f3digo DEBE tener un criterio de desempate (ej: alfab\u00e9tico) o los tests fallar\u00e1n aleatoriamente.\"\n",
    "            )\n",
    "\n",
    "\n",
    "def discover_anomalies(file_path, sample_size=15000):\n",
    "    print(f\"--- Iniciando An\u00e1lisis Profundo: {file_path} (n={sample_size}) ---\")\n",
    "\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i < sample_size:\n",
    "                    data.append(line)\n",
    "                else:\n",
    "                    r = random.randint(0, i)\n",
    "                    if r < sample_size:\n",
    "                        data[r] = line\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo {file_path} no existe.\")\n",
    "        return\n",
    "\n",
    "    parsed = []\n",
    "    corrupt_lines = 0\n",
    "    for line in data:\n",
    "        try:\n",
    "            parsed.append(json.loads(line))\n",
    "        except Exception:\n",
    "            corrupt_lines += 1\n",
    "\n",
    "    if corrupt_lines:\n",
    "        print(f\"[ALERTA] Se detectaron {corrupt_lines} l\u00edneas corruptas en la muestra.\")\n",
    "\n",
    "    # Normalizaci\u00f3n para an\u00e1lisis\n",
    "    df = pd.json_normalize(parsed)\n",
    "\n",
    "    print(\"\\n1. INTEGRIDAD DE COLUMNAS CLAVE\")\n",
    "    columns_to_check = [\"date\", \"content\", \"mentionedUsers\", \"user.username\", \"user.id\"]\n",
    "    for col in columns_to_check:\n",
    "        if col in df.columns:\n",
    "            types = df[col].apply(lambda x: type(x).__name__).value_counts()\n",
    "            nulls = df[col].isnull().sum()\n",
    "            print(f\"- '{col}': {len(types)} tipos detectados. Nulos: {nulls}\")\n",
    "        else:\n",
    "            print(f\"- [ERROR] Columna '{col}' NO encontrada.\")\n",
    "\n",
    "    print(\"\\n2. AN\u00c1LISIS DE USUARIOS (Q1 & Q3)\")\n",
    "    # Verificar si un username tiene m\u00faltiples IDs (cambio de handle)\n",
    "    user_consistency = df.groupby(\"user.username\")[\"user.id\"].nunique()\n",
    "    inconsistent = user_consistency[user_consistency > 1]\n",
    "    print(f\"- Usuarios con m\u00e1s de un ID: {len(inconsistent)}\")\n",
    "\n",
    "    # Calcular potencial de String Interning\n",
    "    total_names = len(df[\"user.username\"])\n",
    "    unique_names = df[\"user.username\"].nunique()\n",
    "    print(\n",
    "        f\"- Ratio de Repetici\u00f3n de Usernames: {total_names / unique_names:.2f}x (Alto ratio justifica sys.intern)\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n3. AN\u00c1LISIS DE MENCIONES (Q3)\")\n",
    "    # Validar estructura interna de mentionedUsers\n",
    "    mentions_data = df[\"mentionedUsers\"].dropna()\n",
    "    has_nested_nulls = mentions_data.apply(\n",
    "        lambda x: any(m.get(\"username\") is None for m in x if isinstance(m, dict))\n",
    "    ).sum()\n",
    "    print(f\"- Registros con listas de menciones v\u00e1lidas: {len(mentions_data)}\")\n",
    "    print(f\"- Listas con objetos internos nulos: {has_nested_nulls}\")\n",
    "\n",
    "    print(\"\\n4. AN\u00c1LISIS DE EMOJIS COMPLEJOS (Q2)\")\n",
    "    all_emojis = []\n",
    "    complex_count = 0\n",
    "    for txt in df[\"content\"].dropna():\n",
    "        # emoji_list devuelve informaci\u00f3n detallada de cada emoji\n",
    "        found = emoji.emoji_list(txt)\n",
    "        for e in found:\n",
    "            char = e[\"emoji\"]\n",
    "            all_emojis.append(char)\n",
    "            # Si tiene m\u00e1s de un componente unicode o caracteres especiales de uni\u00f3n\n",
    "            if len(char) > 1 or \"\\u200d\" in char:\n",
    "                complex_count += 1\n",
    "\n",
    "    print(f\"- Total emojis detectados: {len(all_emojis)}\")\n",
    "    print(f\"- Emojis complejos (ZWJ/Multi-char): {complex_count}\")\n",
    "    if all_emojis:\n",
    "        print(f\"- Top 3 Emojis en muestra: {Counter(all_emojis).most_common(3)}\")\n",
    "\n",
    "    print(\"\\n5. VALORES EXTREMOS Y FECHAS\")\n",
    "    df[\"date_parsed\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    print(f\"- Rango temporal: {df['date_parsed'].min()} a {df['date_parsed'].max()}\")\n",
    "    print(f\"- Tweets por fuera de 2021: {len(df[df['date_parsed'].dt.year != 2021])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \ud83d\udd75\ufe0f\u200d\u2642\ufe0f Deep Dive Analysis: ../farmers-protest-tweets-2021-2-4.json ---\n",
      "\n",
      "[1] AN\u00c1LISIS DE TRUNCAMIENTO\n",
      " - La clave 'truncated' NO existe en este dataset (probablemente ya fue procesado/aplanado).\n",
      " - Tweets que terminan visualmente en '\u2026': 4\n",
      "\n",
      "[2] AN\u00c1LISIS DE RETWEETS\n",
      " - Detectados por texto ('RT @...'): 10 (0.05%)\n",
      "   -> CONCLUSI\u00d3N: Los Retweets est\u00e1n presentes. Q2 y Q3 estar\u00e1n inflados por repetici\u00f3n.\n",
      "\n",
      "[3] COMPARATIVA DE MENCIONES (Q3)\n",
      "An\u00e1lisis sobre 1000 registros:\n",
      " - Casos donde Metadata detecta usuarios que Regex NO ve (Hidden/Reply): 14\n",
      "   Ejemplo Hidden -> Texto: '.@RakeshTikaitBKU \u092c\u094b\u0932\u0947- \u0938\u0902\u0938\u0926 \u091c\u093e\u0915\u0930 \u091f\u094d\u0930\u0948\u0915\u094d\u091f\u0930 \u091a\u0932\u093e\u090f\u0902\u0917\u0947...' | Meta: {'KumarKunalmedia', 'RakeshTikaitBKU'}\n",
      " - Casos donde Regex detecta '@' que NO son usuarios v\u00e1lidos en Metadata: 15\n",
      "   Ejemplo Falso Positivo -> Texto: '.@RakeshTikaitBKU \u092c\u094b\u0932\u0947- \u0938\u0902\u0938\u0926 \u091c\u093e\u0915\u0930 \u091f\u094d\u0930\u0948\u0915\u094d\u091f\u0930 \u091a\u0932\u093e\u090f\u0902\u0917\u0947...' | Regex: {'RakeshTikaitBKU', 'kumarkunalmedia'}\n"
     ]
    }
   ],
   "source": [
    "analyze_twitter_nuances(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \ud83d\udd2c An\u00e1lisis Forense Avanzado: ../farmers-protest-tweets-2021-2-4.json ---\n",
      "\n",
      "[1] INTEGRIDAD DE ENTIDADES (Usernames)\n",
      " - Usuarios \u00fanicos (Crudo): 26519\n",
      " - Usuarios \u00fanicos (Normalizado + Lower): 26519\n",
      " -> OK: No se detectaron colisiones, pero es buena pr\u00e1ctica implementarlo.\n",
      "\n",
      "[2] DISTRIBUCI\u00d3N DE ACTIVIDAD (Q1)\n",
      " - Top 5 Usuarios m\u00e1s activos:\n",
      "   [('jot__b', 1019), ('rebelpacifist', 850), ('MaanDee08215437', 830), ('Gurpreetd86', 636), ('GurmVicky', 597)]\n",
      " - El 99% de usuarios tiene menos de 56 tweets.\n",
      " - El usuario #1 tiene 1019 tweets.\n",
      " \u26a0\ufe0f ALERTA: El usuario top tiene una actividad 18.2x mayor al promedio.\n",
      " -> OBSERVACI\u00d3N: Probable Bot. Documentar en SOLUTION.md que esto sesga los resultados.\n",
      "\n",
      "[3] RIESGO DE EMPATES EN EL CORTE\n",
      " - Valor de corte (Puesto #10): 490 tweets\n",
      " - Cu\u00e1ntos usuarios tienen exactamente ese valor: 1\n"
     ]
    }
   ],
   "source": [
    "advanced_analysis(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando An\u00e1lisis Profundo: ../farmers-protest-tweets-2021-2-4.json (n=15000) ---\n",
      "\n",
      "1. INTEGRIDAD DE COLUMNAS CLAVE\n",
      "- 'date': 1 tipos detectados. Nulos: 0\n",
      "- 'content': 1 tipos detectados. Nulos: 0\n",
      "- 'mentionedUsers': 2 tipos detectados. Nulos: 10130\n",
      "- 'user.username': 1 tipos detectados. Nulos: 0\n",
      "- 'user.id': 1 tipos detectados. Nulos: 0\n",
      "\n",
      "2. AN\u00c1LISIS DE USUARIOS (Q1 & Q3)\n",
      "- Usuarios con m\u00e1s de un ID: 0\n",
      "- Ratio de Repetici\u00f3n de Usernames: 2.20x (Alto ratio justifica sys.intern)\n",
      "\n",
      "3. AN\u00c1LISIS DE MENCIONES (Q3)\n",
      "- Registros con listas de menciones v\u00e1lidas: 4870\n",
      "- Listas con objetos internos nulos: 0\n",
      "\n",
      "4. AN\u00c1LISIS DE EMOJIS COMPLEJOS (Q2)\n",
      "- Total emojis detectados: 5547\n",
      "- Emojis complejos (ZWJ/Multi-char): 1188\n",
      "- Top 3 Emojis en muestra: [('\ud83d\ude4f', 645), ('\ud83d\ude02', 401), ('\ud83d\ude9c', 338)]\n",
      "\n",
      "5. VALORES EXTREMOS Y FECHAS\n",
      "- Rango temporal: 2021-02-12 01:36:49+00:00 a 2021-02-24 09:21:51+00:00\n",
      "- Tweets por fuera de 2021: 0\n"
     ]
    }
   ],
   "source": [
    "discover_anomalies(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definici\u00f3n de estaratefias de optimizaci\u00f3n\n",
    "\n",
    "Depues de analizar los datos y el c\u00f3digo,ver detalle en la secci\u00f3n 3 del archivo [SOLUTION.md](SOLUTION.md) \n",
    "\n",
    "\n",
    "## Calidad de software\n",
    "\n",
    "Se proponen realizar test basados en los resultados del analisis de los datos. ver detalle en la secci\u00f3n 4 del archivo [SOLUTION.md](SOLUTION.md) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
