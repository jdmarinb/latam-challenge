# Data Engineer Challenge Solution

Como el reto no tiene mucho c√≥digo, todo se documenta aqu√≠ en un solo archivo para facilitar la lectura. Para mejorar la redacci√≥n y evitar la redundancia se hace uso de IA para mejora rla docuemntaci√≥n.

## 1. Ambiente de Desarrollo

El reto pide buenas pr√°cticas de git. Para eso, lo primero es que cualquiera pueda configurar el ambiente f√°cil:

```bash
make setup
```

### 1.1. Automatizaci√≥n (`Makefile`)
Centraliza los comandos del proyecto (`setup`, `lint`, `test`, `clean`) para que todos hagan lo mismo de la misma forma. Es robusto, agn√≥stico al lenguaje y est√° en casi todos los sistemas. En Windows se instala con `scoop install make` o revisando la [documentaci√≥n oficial](https://www.gnu.org/software/make/).

### 1.2. Entorno Virtual (`uv`)
Maneja ambientes de Python y paquetes. Se usa por su buen rendimiento y porque genera archivos de `lock` y gestiona `.toml` bien en proyectos complejos.

### 1.3. Dependencias
- `requirements.txt`: paquetes del proyecto
- `requirements-dev.txt`: dependencias de desarrollo (linters, testing) + invoca `requirements.txt` para que el ambiente de desarrollo sea un superconjunto del de producci√≥n

### 1.4. Git Hooks (`pre-commit`)
Automatiza revisi√≥n y formateo *antes* del commit. El c√≥digo que llega al repo cumple los est√°ndares.

**Herramientas:**
- `Ruff`: linter y formateador que detecta errores y code smells
- `detect-secrets`: previene que API keys, passwords, etc. se suban al repo

### 1.5. Commits Convencionales (`Commitizen`)
Fuerza formato est√°ndar en los commits. Mejora la legibilidad del historial, facilita changelogs y permite automatizar versiones.

```bash
make commit  # en lugar de git commit
```

### 1.6. Flujo Git (GitFlow simplificado)
- `main`: versi√≥n estable de producci√≥n
- `develop`: rama principal donde se integra todo
- `feature/<nombre>`: ramas para nuevas funcionalidades, creadas desde `develop`

Mantiene historial limpio y facilita tanto desarrollo como mantenimiento.

## 2. Exploraci√≥n de Datos

Archivo: `farmers-protest-tweets-2021-2-4.json`

### 2.1. Estructura
- **Formato**: JSONL (cada l√≠nea es un JSON independiente)
- **Volumen**: 117,407 registros (~400MB+)
- **Campos clave**:
  - `date`: timestamp del tweet ‚Üí **Q1**
  - `content`: texto con emojis ‚Üí **Q2**
  - `user.username`: usuario que tweetea ‚Üí **Q1**
  - `mentionedUsers`: lista de usuarios mencionados ‚Üí **Q3**

### 2.2. Identificadores
- `id`: num√©rico de 64 bits, identifica cada tweet √∫nico
- `url`: tambi√©n sirve como identificador √∫nico
- **Usuario**: Tiene `user.id` (inmutable) y `user.username` (puede cambiar seg√∫n documentaci√≥n de Twitter). Un usuario puede aparecer m√∫ltiples veces (varios tweets)
- **Importante para Q1**: Se agrupa internamente por `user.id` para evitar fragmentar m√©tricas si alguien cambi√≥ su nombre durante el periodo. Aunque el reto pide devolver `username` (string), primero se cuenta por ID y luego se mapea al username m√°s reciente

### 2.3. Variables necesarias para el an√°lisis

Teniendo en cuenta que cada pregunta requiere un tipo de an√°lisis diferente:

**Q1.** Las top 10 fechas donde hay m√°s tweets. Mencionar el usuario (username) que m√°s publicaciones tiene por cada uno de esos d√≠as.
```python
[(datetime.date(1999, 11, 15), "LATAM321"), (datetime.date(1999, 7, 15), "LATAM_CHI"), ...]‚Äã
```

**Q2.** Los top 10 emojis m√°s usados con su respectivo conteo.
```python
[("‚úàÔ∏è", 6856), ("‚ù§Ô∏è", 5876), ...]
```

**Q3.** El top 10 hist√≥rico de usuarios (username) m√°s influyentes en funci√≥n del conteo de las menciones (@) que registra cada uno de ellos.
```python
[("LATAM321", 387), ("LATAM_CHI", 129), ...]
```

Variables necesarias para responder cada pregunta:

| Pregunta | Variable | Uso |
| :--- | :--- | :--- |
| **Q1** | `date` | Agrupaci√≥n cronol√≥gica por d√≠a. |
| **Q1** | `user.id` | Identificaci√≥n del autor para el conteo por d√≠a (inmutable). |
| **Q1** | `user.username` | Mapeo final para devolver el string solicitado. |
| **Q2** | `content` | Extracci√≥n de emojis del texto. |
| **Q3** | `mentionedUsers` | Lista de usuarios mencionados. |
| **Q3** | `mentionedUsers.username` | Identificaci√≥n del usuario mencionado para el conteo hist√≥rico. |

### 2.4. Errores y Casos Borde

La explicaci√≥n del objeto tweet est√° en la [documentaci√≥n oficial de Twitter](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/overview/tweet-object). Complementando con investigaci√≥n usando IA (Gemini), se identifican casos de negocio comunes:

**A. El problema de la "Doble Identidad" (Q1)**

La documentaci√≥n de Twitter indica que un usuario puede cambiar su `username` (handle), pero su `id` es inmutable. Si un usuario cambi√≥ su nombre durante el periodo del dataset, contarlo por `username` podr√≠a fragmentar sus m√©tricas.

**Decisi√≥n**: Aunque el reto pide devolver el `username`, internamente se agrupa por `id` y luego se mapea al `username` m√°s reciente.

**B. El campo `mentionedUsers` vs. @texto (Q3)**

El reto pide menciones basadas en el conteo de `@`. Sin embargo, el campo `mentionedUsers` es un objeto enriquecido por Twitter que ya parse√≥ el texto.

**Caso Borde**: ¬øQu√© pasa si el texto dice `@usuario` pero el objeto `mentionedUsers` es `null` (com√∫n en tweets borrados o cuentas suspendidas)?

**Decisi√≥n**: Se usa el campo estructurado `mentionedUsers` (m√°s eficiente y captura menciones "invisibles" como hidden/reply) en lugar de regex sobre el texto. Si es `null`, el tweet no cuenta menciones aunque el texto tenga `@`.

**C. La trampa de los Emojis Compuestos (Q2)**

Los ZWJ (Zero Width Joiners) unen m√∫ltiples emojis. Ejemplo: el emoji de "Familia" üë®‚Äçüë©‚Äçüëß‚Äçüë¶ es en realidad 4 emojis unidos por caracteres invisibles.

**Decisi√≥n**: Se usa `emoji.analyze()` con soporte para secuencias ZWJ completas. De lo contrario, el top 10 se llenar√° de "hombres", "mujeres" y "ni√±os" individuales en lugar de "familia".

**D. Q1: El desaf√≠o del "Usuario con m√°s tweets por d√≠a"**

Esta pregunta es un Top-N de un Top-N:
1. Encontrar los Top 10 d√≠as con m√°s tweets
2. Para cada uno de esos 10 d√≠as, encontrar el usuario m√°s activo

**Decisi√≥n**: Optimizaci√≥n en 2 pasos para no guardar diccionario gigante `{fecha: {usuario: cuenta}}`:
- Paso 1: Contar tweets por fecha y obtener Top 10 fechas
- Paso 2: Solo para esas 10 fechas, acumular conteos de usuarios

**E. La Pesadilla de Unicode (Normalizaci√≥n NFKC)**

En Q1 (usernames) y Q2 (emojis), Python puede traicionar si no se normaliza Unicode.

**El Problema**: El car√°cter `√±` se puede escribir de dos formas en bytes:
1. Como un solo car√°cter (U+00F1)
2. Como `n` + s√≠mbolo `~` (U+006E + U+0303)

**El Riesgo**: Para Python, `"Pe√±a" != "Pe√±a"`. Si no se normaliza, `collections.Counter` los cuenta como dos usuarios o emojis distintos, fragmentando el Top 10.

**Decisi√≥n**: Aplicar `unicodedata.normalize('NFKC', text)` antes de contar.

**F. Comportamiento de Bots (Outliers en Q1)**

Q1 pide el usuario con m√°s tweets por d√≠a. En eventos como el Farmers Protest hay muchos bots.

**El Riesgo**: Si un usuario tiene 5,000 tweets en un d√≠a (bot haciendo spam), t√©cnicamente es el "Top 1", pero ensucia el an√°lisis.

**Decisi√≥n**: Se reportan tal cual lo pide el enunciado, pero se verifica la distribuci√≥n de tweets por usuario. Si el Top 1 tiene 100x m√°s tweets que el Top 2, se menciona en documentaci√≥n como observaci√≥n de calidad de datos.

**G. Case Sensitivity (Q1 y Q3)**

Twitter trata los usernames como case-insensitive, pero los datos crudos pueden variar.

**El Riesgo**: `@Latam321` y `@latam321` son la misma persona. Si se cuentan strings crudos, se divide su influencia.

**Decisi√≥n**: Convertir siempre a `.lower()` antes de agrupar en Q1 y Q3.

**H. Determinismo en Empates (Sorting)**

**El Problema**: El reto pide "Top 10". ¬øQu√© pasa si el usuario #10 y el #11 tienen exactamente 50 tweets?

**El Riesgo**: El ordenamiento est√°ndar de Python (`sort`) es estable, pero sin criterio de desempate expl√≠cito, el c√≥digo podr√≠a devolver resultados diferentes en ejecuciones distintas en sistemas distribuidos.

**Decisi√≥n**: Implementar criterio de ordenamiento secundario: `sort(key=lambda x: (-count, username))` para garantizar determinismo.

Para analizar estos y otros casos presentes en los datos, se realiza una exploraci√≥n de una muestra en [challenge.ipynb](src/challenge.ipynb).

Tras un an√°lisis estad√≠stico de 10,000 registros, se identificaron los siguientes casos cr√≠ticos:

**An√°lisis Estad√≠stico de Calidad (n=10,000):**
- **Integridad Estructural**: No se detectaron discrepancias de tipos en `date` (100% string) o `content` (100% string)
- **Menciones (Q3)**: El campo `mentionedUsers` es altamente vol√°til. El 67.28% de los tweets son `null` (NoneType), mientras que el 32.72% son listas. No se detectaron listas vac√≠as (`[]`)
- **Complejidad de Texto (Q2)**: Se detectaron 821 emojis complejos (multi-char/compound). Emojis como `üôèüèª` (manos en oraci√≥n con tono de piel) o banderas como `üáÆüá≥` requieren tratamiento de grafemas
- **Extremos de Contenido**: Tweets desde 15 hasta 852 caracteres, implica que el parsing de texto debe ser eficiente
- **Ratio de Repetici√≥n de Usuarios**: 2.20x, justifica el uso de `sys.intern()` para optimizar memoria

**Casos Borde y de Negocio Identificados:**

| Variable | Caso de Prueba / Anomal√≠a | Frecuencia / Ejemplo | Estrategia de Mitigaci√≥n |
| :--- | :--- | :--- | :--- |
| `date` | Desfase de Zona Horaria | `2021-02-21T23:39:32+00:00` | Normalizaci√≥n mandatoria a UTC `.date()` para evitar saltos de d√≠a. |
| `content` | Emojis con ZWJ (Zero Width Joiners) | `üë®‚Äçüë©‚Äçüëß‚Äçüë¶` = 4 emojis unidos | Uso de `emoji.analyze()` con soporte para secuencias ZWJ completas. |
| `content` | Multiling√ºismo (Hindi/Punjabi) | `.@RakeshTikaitBKU ‡§¨‡•ã‡§≤‡•á- ‡§∏‡§Ç‡§∏‡§¶ ‡§ú‡§æ‡§ï‡§∞...` | El parser debe soportar codificaci√≥n `utf-8` para no corromper texto en otros idiomas. |
| `content` | Truncamiento (Legacy) | Clave `truncated` no existe | El dataset parece pre-procesado (aplanado), pero se valida fin de texto `‚Ä¶`. |
| `mentionedUsers` | Menciones "Invisibles" | Hidden/Reply | Uso de metadata `mentionedUsers` en lugar de Regex para capturar menciones que no est√°n en el texto visible. |
| `mentionedUsers` | Referencia Nula vs @texto | `null` vs `@falso_positivo` | Se prioriza metadata oficial sobre Regex para evitar falsos positivos de texto. |
| `user.username` | Ratio de Repetici√≥n | 2.20x | Justifica el uso de `sys.intern()` para optimizar memoria RAM. |
| `all` | Retweets (Duplicidad) | `RT @...` (0.05% muestra) | Interpretaci√≥n estricta: se cuentan los emojis/menciones de RTs como publicaciones nuevas. |

**Conclusiones:**
- El archivo es grande (~400MB+). Leerlo completo en memoria (`json.load`) puede causar problemas en recursos limitados
- **Recomendaci√≥n**: procesamiento por streaming (l√≠nea por l√≠nea) para optimizar memoria
- La estructura anidada (`user`, `mentionedUsers`) requiere navegaci√≥n cuidadosa del JSON para evitar errores por campos nulos

## 3. Estrategias de Optimizaci√≥n y Arquitectura

Una de las estrategias centrales es aprovechar capacidades nativas y librer√≠as optimizadas para maximizar la eficiencia en ambos frentes:

- **String Interning (`sys.intern`)**:
  - **Memoria**: Basado en un **ratio de repetici√≥n de 2.20x**, esta t√©cnica fuerza a Python a reutilizar el mismo objeto en RAM para valores id√©nticos. Reduce el *footprint* al evitar duplicados de texto denso en los diccionarios de Q1 y Q3.
  - **Tiempo**: Acelera los agrupamientos y b√∫squedas, ya que las comparaciones entre strings "internados" se realizan por direcci√≥n de memoria (punteros) en lugar de evaluar car√°cter por car√°cter.

- **Normalizaci√≥n On-the-fly**:
  - **Memoria**: Al aplicar Unicode NFKC y conversi√≥n a min√∫sculas *durante* la lectura, evitamos duplicar estructuras de datos (versi√≥n original vs procesada).
  - **Tiempo**: Implementa un patr√≥n de **Single Pass** (pasada √∫nica), eliminando la latencia de recorrer el dataset m√∫ltiples veces para limpieza y agregaci√≥n.


### 3.1. Optimizaci√≥n de Tiempo (Latencia)

**`polars`:** Al representar un archivo `Json` de manera columnar atravez de Apache Arrow lo que permite hacer **Projection Pushdown** y **Predicate Pushdown** automaticamente; en su modo Lazy. Por lo que sera una opci√≥n a considerar principalmente en la optimizaci√≥n de tiempo. Al ser **Multi-thread** divide el archivo en __"Chunks"__ de tama√±o √≥ptimo para maximizar el uso de los hilos del CPU sin saturar la comunicaci√≥n. Adicionalmente al ser **Single Node** (los hilos intercambian datos en la RAM) y no distribuido como Spark (los nodos intercambian dato spor la RED), el shuffle no es tan costoso, lo qeu lo hace ideal para este caso en que mos archivos noestan particionados (JSON). Adicionalmente al estar programado en `rust` es multihilo loq eu lo hace m√°s eficiente para procesamiento en paralelo que usar `ProcessPoolExecutor`. Adicionalmente s epeude aprovechar la vectoricaci√≥n + SIMD nativa del paquete, pese a no ser ser el mejor escenario (operaciones n√∫mericas), agrega eficiencia por ejemplo al realizar funciones de agregaci√≥n. Adem√°s de esto como cualquier motor de procesamiento por columnas se ve beneficiado cuando se le indica el esquema de las varaibles en lugar de dejar que este las infiera.

### 3.2. Optimizaci√≥n de Memoria (Footprint)

**`orjson`:** Es un motor de parsing y serializaci√≥n JSON de alto rendimiento implementado en Rust. Supera al m√≥dulo est√°ndar de Python al procesar directamente estructuras de bytes y datetime, eliminando el overhead de decodificaci√≥n y transformaci√≥n de tipos. Reduce significativamente la latencia de CPU en la deserializaci√≥n y habilita un patr√≥n de lectura en streaming (procesamiento secuencial). Al usar algoritmos **Single Pass** permite iterar sobre grandes vol√∫menes de datos manteniendo una huella de memoria (memory footprint) constante y m√≠nima, independientemente del tama√±o total del archivo. Combinado con programaci√≥n funcional  **`itertools`** **`collections.Counter`**  Evita crear listas intermedias de objetos. `Counter` es un `dict` optimizado en C.


### 3.3. Decisiones de Dise√±o por Pregunta

| Pregunta | Estrategia T√©cnica | Justificaci√≥n Basada en Exploraci√≥n |
| :--- | :--- | :--- |
| **Q1** | Agrupaci√≥n H√≠brida (ID + Map) | Evita fragmentaci√≥n por cambio de usernames; garantiza integridad del conteo hist√≥rico. |
| **Q2** | An√°lisis de Grafemas (ZWJ) librer√≠a **`emoji`** | Detectado en exploraci√≥n (1,188 emojis complejos); garantiza que `üë®‚Äçüë©‚Äçüëß‚Äçüë¶` no se cuente como 4 personas. |
| **Q3** | Metadata-First (No Regex) | Evita 15% de falsos positivos y captura 14% de menciones "invisibles" (replies/metadata). |

**Nota:** En la pregunta 2 se utiliza la librer√≠a **`emoji`**, aportando la garant√≠a t√©cnica para manejar secuencias ZWJ (Zero Width Joiners) y modificadores de tono de piel. Sin ella, los conteos de Q2 ser√≠an err√≥neos al fragmentar grafemas. Adem√°s sustituye el uso de expresiones regulares (Regex) masivas por algoritmos de b√∫squeda optimizados, reduciendo el costo de CPU asociado al *backtracking* de patrones complejos.

## 4. Benchmarks

Para medir el rendimiento del algoritmo en tiempo y memoria se utilizan las librerias `memory_profiler` y `cProfile` y `pstats` para medir los tiempos como se sugeria en el  [README.md](README.md). Haciendo uso de esta misma funci√≥n optimiz√≥ cada parte del codigo.

### 4.1 Lectura de Archivo
Lo primero a optimizar es la lectura de los datos, comparando todas las metodolog√≠as de ingesta evaluadas para el dataset de 400MB.

| M√©todo | Tiempo Real (s) | RAM Pico (MB) | Retorno | Veredicto |
| :--- | :--- | :--- | :--- | :--- |
| **Polars Lazy + Schema** | **~0.00** | **~3138** | LazyFrame | **1¬∞ Tiempo 1¬∞ Memoria**. (Lazy). |
| **Streaming/Chunks Orjson** | **~0.00** | **~3776** | Generador | **1¬∞ Tiempo 2¬∞ Memoria** (Lazy). |
| **Polars Eager + Schema** | **~0.33** | **~4761** | DataFrame | Bueno. (Materialized)|
| **Polars Lazy (Scan)** | ~0.40 | ~4763 | LazyFrame | Bueno (Inferencia). |
| **Polars Eager (Read)** | ~6.65 | ~5121 | DataFrame | Lento (Inferencia). |
| **Standard JSON** | ~9.90 | ~4500 | Lista | Lento (Materialized). |
| **Pandas read_json** | ~10.41 | ~5492 | DataFrame | **Inadecuado** (OOM Risk). |
| **Full Memory** | ~11.06 | ~4257 | Lista | Ineficiente (Redundancia). |

*\*Polars requiere esquemas fijos (`pl.Struct`) para evitar errores de inferencia.*

**Nota sobre Polars**: Debido a que la versi√≥n **Lazy** no materializa los datos de forma inmediata (solo prepara el plan de ejecuci√≥n), el benchmark reporta tiempos de 0.00s. Se identific√≥ que al definir el esquema, Polars Lazy reduce su pico de memoria a **~3.1GB**, siendo el m√©todo m√°s eficiente dentro de la suite de Polars para la gesti√≥n de recursos.

#### An√°lisis Detallado de Rendimiento (Python Profilers)
Tras aplicar `cProfile` y `pstats` en un laboratorio modular, se identificaron los cuellos de botella reales:
- **Overhead de `raw_decode`**: El perfilado de `Standard JSON` revel√≥ que el ~60% del tiempo acumulado (~6.3s de 10.3s totales del profiler) se consume en la funci√≥n interna de decodificaci√≥n de strings, justificando el paso a motores de Rust/C como `orjson`.
- **Overhead de Instrumentaci√≥n**: Se confirm√≥ que el uso de profilers a√±ade un retraso artificial (ej. de 9.9s real a 11.8s en ejecuci√≥n profilada), por lo que las m√©tricas finales se basan en mediciones de tiempo "Wall-clock" desacopladas.


### 4.2. Procesamiento

Para el procesamiento se evaluan las opciones de lectura elegdas **`polars`** con schema en sus versiones **Lazy** y **Eager** y el procesamiento en formato vectorial, y por otro lado **`orJson`** con procesamiento en el paradigma **funcional**. Ambos metodos en *streamig* y en *batch*.






## 5. Calidad de Software

### 5.1. Manejo de Errores

Basado en el an√°lisis del dataset, se implementan las siguientes estrategias:

**1. Resiliencia al Parsing (JSONL Corruption):**
- **Riesgo**: L√≠neas truncadas o caracteres de escape mal procesados en archivo de 400MB+
- **Soluci√≥n**: Bloque `try-except` alrededor de `orjson.loads()`/`ujson.loads()`. Las l√≠neas corruptas se registran en log pero no detienen la ejecuci√≥n

**2. Esquema de Usuario Inconsistente:**
- **Riesgo**: El objeto `user` es anidado y campos podr√≠an ser nulos
- **Soluci√≥n**: Acceso seguro `tweet.get('user', {}).get('username')`. Si es nulo, se descarta para Q1 y Q3

**3. Manejo de Menciones Vac√≠as:**
- **Hallazgo**: 67.28% de tweets tienen `mentionedUsers` en `null`
- **Soluci√≥n**: Validar `if tweet.get('mentionedUsers') is not None` antes de iterar

**4. Codificaci√≥n de Emojis y Caracteres Especiales:**
- **Hallazgo**: 821 emojis compuestos detectados en muestra + caracteres multiling√ºes (Hindi, Punjabi)
- **Soluci√≥n**: Librer√≠a `emoji` con soporte para `grapheme clusters`. Un emoji compuesto se cuenta como 1 unidad

**5. Normalizaci√≥n de Fechas:**
- **Hallazgo**: Formato ISO 8601 incluye offsets de zona horaria
- **Soluci√≥n**: Extraer solo `.date()` para agrupamiento de Q1

### 4.2. Estrategia de Testing Robustecida

Dada la naturaleza vol√°til de los datos sociales, se implementa una suite de pruebas basada en casos reales detectados:

| Categor√≠a | Caso de Prueba | Entrada Esperada (Mock) | Resultado Esperado | Justificaci√≥n |
| :--- | :--- | :--- | :--- | :--- |
| **Integridad** | JSON Corrupto | `{"date": "2021...", [TRUNCATED]` | Skip / Log error | Evitar ca√≠da del pipeline por truncamiento de archivo. |
| **Q1: Fechas** | Cambio de D√≠a UTC | `2021-02-12T23:59:59+00:00` | Agrupar en `2021-02-12` | Garantizar que el offset no mueva tweets a d√≠as incorrectos. |
| **Q1: Usuarios** | Cambio de Username | `id: 1, user: A` -> `id: 1, user: B` | Conteo √∫nico para `id: 1` | Evitar fragmentaci√≥n de m√©tricas de usuarios activos. |
| **Q2: Emojis** | Secuencias ZWJ | `üë®‚Äçüë©‚Äçüëß‚Äçüë¶` (Familia) | Count = 1 | Evitar sobreconteo de personas individuales en emojis compuestos. |
| **Q2: Emojis** | Modificadores Tono | `üôèüèª` (Manos + Tono) | Count = 1 | Tratar variaciones de tono como una sola unidad visual. |
| **Q3: Menciones** | Menci√≥n Hidden | `.@user` (no al inicio) | Detectar `@user` | Validar que el parser captura menciones en cualquier posici√≥n. |
| **Q3: Menciones** | Metadata vs Regex | `RT @user` | Usar metadata, no Regex | Evitar falsos positivos de texto que no son usuarios v√°lidos. |
| **Rendimiento** | Archivo Vac√≠o | (Archivo de 0 bytes) | `[]` (Empty List) | Manejo elegante de fuentes de datos sin registros. |
| **Consistencia** | Paridad Funcional | Dataset de 1,000 registros | `time == memory` | Garantizar que ambas optimizaciones devuelven el mismo Top 10. |

### 5.3. Herramientas de Calidad
- **`pytest`**: Ejecuci√≥n de la matriz de pruebas anterior.
- **`Ruff`**: Garantiza que el c√≥digo sigue est√°ndares de la comunidad (PEP8) y est√° libre de *dead code*.
- **`detect-secrets`**: Escaneo preventivo para asegurar que no se filtren credenciales de la API de Twitter en los scripts.


### Escalamiento a Big Data

Si los datos escalaran a billones de registros, se aplicar√≠an estrategias adicionales que requieren infraestructura y cambio de tecnologias, probablemente spark o flink:

**1. Formato Columnar (Parquet/Avro):**
- Permite leer solo columnas necesarias (ej: solo `date` y `user`)
- Compresi√≥n de strings repetitivos (usernames)
- Reduce dr√°sticamente I/O y memoria

**2. Particionamiento:**
- Particionar f√≠sicamente por `year/month/day`
- El motor ignora carpetas que no coinciden con el rango consultado

**3. Probabilistic Data Structures:**
- **HyperLogLog**: conteos de cardinalidad (Q3) con error m√≠nimo
- **Count-Min Sketch**: frecuencias de top-k (Q2)
- Memoria fija (KB en lugar de GB)

**4. Pre-agregaci√≥n:**
- ETL que genere agregados horarios/diarios
- Consultas de "Top 10" instant√°neas al leer pre-calculados
