{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu soluci√≥n y todas las suposiciones que est√°s considerando. Aqu√≠ puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import emoji\n",
    "import random\n",
    "import orjson\n",
    "import unicodedata\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "El archivo [SOLUTION.md](SOLUTION.md) contiene la explicaci√≥n detallada del paso apaso de la soluci√≥n\n",
    "\n",
    "## Ambiente de desarrollo\n",
    "\n",
    "Se utiliza make para desarrollar el proyecto. Puedes ver las opciones disponibles en el archivo [Makefile](Makefile).\n",
    "\n",
    "## Data Analisis\n",
    "\n",
    "Se realiza un analisis de los datos para comprender su estructura y posibles problemas de calidad. ver detalle en la secci√≥n 2 del archivo [SOLUTION.md](../SOLUTION.md) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_twitter_nuances(file_path, sample_size=20000):\n",
    "    print(f\"--- üïµÔ∏è‚Äç‚ôÇÔ∏è Deep Dive Analysis: {file_path} ---\")\n",
    "\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= sample_size:\n",
    "                    break\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Archivo no encontrado.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # ==========================================\n",
    "    # 1. AN√ÅLISIS DE TRUNCAMIENTO (Extended Mode)\n",
    "    # ==========================================\n",
    "    print(\"\\n[1] AN√ÅLISIS DE TRUNCAMIENTO\")\n",
    "\n",
    "    # Verificar si existen claves nativas de la API v1.1\n",
    "    has_truncated_key = \"truncated\" in df.columns\n",
    "    has_extended_tweet = \"extended_tweet\" in df.columns\n",
    "\n",
    "    if has_truncated_key:\n",
    "        truncated_count = df[\"truncated\"].sum() if df[\"truncated\"].dtype == bool else 0\n",
    "        print(\n",
    "            f\" - Tweets marcados como 'truncated': {truncated_count} ({truncated_count / len(df):.2%})\"\n",
    "        )\n",
    "\n",
    "        if has_extended_tweet:\n",
    "            extended_count = df[\"extended_tweet\"].notnull().sum()\n",
    "            print(f\" - Tweets con objeto 'extended_tweet' disponible: {extended_count}\")\n",
    "    else:\n",
    "        print(\n",
    "            \" - La clave 'truncated' NO existe en este dataset (probablemente ya fue procesado/aplanado).\"\n",
    "        )\n",
    "\n",
    "    # Verificar visualmente si el contenido parece cortado\n",
    "    # Los tweets truncados suelen terminar en \"...\" o un enlace t.co\n",
    "    df[\"ends_with_ellipsis\"] = df[\"content\"].astype(str).str.strip().str.endswith(\"‚Ä¶\")\n",
    "    suspicious_truncation = df[\"ends_with_ellipsis\"].sum()\n",
    "    print(f\" - Tweets que terminan visualmente en '‚Ä¶': {suspicious_truncation}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. AN√ÅLISIS DE RETWEETS (Duplicidad)\n",
    "    # ==========================================\n",
    "    print(\"\\n[2] AN√ÅLISIS DE RETWEETS\")\n",
    "\n",
    "    # Detectar RTs\n",
    "    # Opci√≥n A: Clave 'retweeted_status' (Standard API)\n",
    "    if \"retweeted_status\" in df.columns:\n",
    "        rts_count = df[\"retweeted_status\"].notnull().sum()\n",
    "        print(f\" - Detectados por metadato 'retweeted_status': {rts_count}\")\n",
    "\n",
    "    # Opci√≥n B: Texto empieza con \"RT @\"\n",
    "    df[\"is_rt_text\"] = df[\"content\"].astype(str).str.startswith(\"RT @\")\n",
    "    rts_text_count = df[\"is_rt_text\"].sum()\n",
    "    print(\n",
    "        f\" - Detectados por texto ('RT @...'): {rts_text_count} ({rts_text_count / len(df):.2%})\"\n",
    "    )\n",
    "\n",
    "    if rts_text_count > 0:\n",
    "        print(\n",
    "            \"   -> CONCLUSI√ìN: Los Retweets est√°n presentes. Q2 y Q3 estar√°n inflados por repetici√≥n.\"\n",
    "        )\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. MENCIONES: TEXTO VS METADATA (Q3)\n",
    "    # ==========================================\n",
    "    print(\"\\n[3] COMPARATIVA DE MENCIONES (Q3)\")\n",
    "\n",
    "    # Funci√≥n para extraer menciones con Regex (enfoque ingenuo)\n",
    "    def extract_regex_mentions(text):\n",
    "        return set(re.findall(r\"@(\\w+)\", str(text)))\n",
    "\n",
    "    # Funci√≥n para extraer menciones de Metadata (enfoque robusto)\n",
    "    def extract_meta_mentions(mentions_list):\n",
    "        if not isinstance(mentions_list, list):\n",
    "            return set()\n",
    "        return set(\n",
    "            m.get(\"username\")\n",
    "            for m in mentions_list\n",
    "            if isinstance(m, dict) and m.get(\"username\")\n",
    "        )\n",
    "\n",
    "    # Aplicar comparativa en una muestra peque√±a para velocidad\n",
    "    sample_df = df.head(1000).copy()\n",
    "\n",
    "    sample_df[\"regex_mentions\"] = sample_df[\"content\"].apply(extract_regex_mentions)\n",
    "    sample_df[\"meta_mentions\"] = sample_df[\"mentionedUsers\"].apply(\n",
    "        extract_meta_mentions\n",
    "    )\n",
    "\n",
    "    # Buscar discrepancias\n",
    "    # Casos donde Metadata tiene ALGO pero Regex NO (Menciones invisibles/Reply)\n",
    "    sample_df[\"hidden_mentions\"] = sample_df.apply(\n",
    "        lambda x: x[\"meta_mentions\"] - x[\"regex_mentions\"], axis=1\n",
    "    )\n",
    "    hidden_count = sample_df[sample_df[\"hidden_mentions\"].astype(bool)].shape[0]\n",
    "\n",
    "    # Casos donde Regex tiene ALGO pero Metadata NO (Falsos positivos, emails, usuarios suspendidos)\n",
    "    sample_df[\"fake_mentions\"] = sample_df.apply(\n",
    "        lambda x: x[\"regex_mentions\"] - x[\"meta_mentions\"], axis=1\n",
    "    )\n",
    "    fake_count = sample_df[sample_df[\"fake_mentions\"].astype(bool)].shape[0]\n",
    "\n",
    "    print(f\"An√°lisis sobre {len(sample_df)} registros:\")\n",
    "    print(\n",
    "        f\" - Casos donde Metadata detecta usuarios que Regex NO ve (Hidden/Reply): {hidden_count}\"\n",
    "    )\n",
    "    if hidden_count > 0:\n",
    "        example = sample_df[sample_df[\"hidden_mentions\"].astype(bool)].iloc[0]\n",
    "        print(\n",
    "            f\"   Ejemplo Hidden -> Texto: '{example['content'][:50]}...' | Meta: {example['meta_mentions']}\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\" - Casos donde Regex detecta '@' que NO son usuarios v√°lidos en Metadata: {fake_count}\"\n",
    "    )\n",
    "    if fake_count > 0:\n",
    "        example = sample_df[sample_df[\"fake_mentions\"].astype(bool)].iloc[0]\n",
    "        print(\n",
    "            f\"   Ejemplo Falso Positivo -> Texto: '{example['content'][:50]}...' | Regex: {example['regex_mentions']}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def advanced_analysis(file_path):\n",
    "    print(f\"--- üî¨ An√°lisis Forense Avanzado: {file_path} ---\")\n",
    "\n",
    "    usernames_raw = []\n",
    "    usernames_normalized = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    tweet = json.loads(line)\n",
    "                    u = tweet.get(\"user\", {}).get(\"username\")\n",
    "                    if u:\n",
    "                        usernames_raw.append(u)\n",
    "                        # Normalizaci√≥n NFKC + Lowercase\n",
    "                        usernames_normalized.append(\n",
    "                            unicodedata.normalize(\"NFKC\", u).lower()\n",
    "                        )\n",
    "                except Exception:\n",
    "                    continue\n",
    "    except FileNotFoundError:\n",
    "        print(\"Archivo no encontrado\")\n",
    "        return\n",
    "\n",
    "    # 1. CHECK DE CASE SENSITIVITY Y UNICODE\n",
    "    unique_raw = len(set(usernames_raw))\n",
    "    unique_norm = len(set(usernames_normalized))\n",
    "\n",
    "    print(\"\\n[1] INTEGRIDAD DE ENTIDADES (Usernames)\")\n",
    "    print(f\" - Usuarios √∫nicos (Crudo): {unique_raw}\")\n",
    "    print(f\" - Usuarios √∫nicos (Normalizado + Lower): {unique_norm}\")\n",
    "    diff = unique_raw - unique_norm\n",
    "    if diff > 0:\n",
    "        print(\n",
    "            f\" ‚ö†Ô∏è ALERTA: Se detectaron {diff} duplicados por falta de normalizaci√≥n/may√∫sculas.\"\n",
    "        )\n",
    "        print(\" -> ACCI√ìN: Es MANDATORIO aplicar .lower() y unicodedata.\")\n",
    "    else:\n",
    "        print(\n",
    "            \" -> OK: No se detectaron colisiones, pero es buena pr√°ctica implementarlo.\"\n",
    "        )\n",
    "\n",
    "    # 2. DETECCI√ìN DE BOTS (OUTLIERS Q1)\n",
    "    print(\"\\n[2] DISTRIBUCI√ìN DE ACTIVIDAD (Q1)\")\n",
    "    counts = Counter(usernames_raw)\n",
    "    top_5 = counts.most_common(5)\n",
    "\n",
    "    df = pd.Series(list(counts.values()))\n",
    "    p99 = df.quantile(0.99)\n",
    "    max_tweets = df.max()\n",
    "\n",
    "    print(f\" - Top 5 Usuarios m√°s activos:\\n   {top_5}\")\n",
    "    print(f\" - El 99% de usuarios tiene menos de {p99:.0f} tweets.\")\n",
    "    print(f\" - El usuario #1 tiene {max_tweets} tweets.\")\n",
    "\n",
    "    if max_tweets > (p99 * 10):\n",
    "        print(\n",
    "            f\" ‚ö†Ô∏è ALERTA: El usuario top tiene una actividad {max_tweets / p99:.1f}x mayor al promedio.\"\n",
    "        )\n",
    "        print(\n",
    "            \" -> OBSERVACI√ìN: Probable Bot. Documentar en SOLUTION.md que esto sesga los resultados.\"\n",
    "        )\n",
    "\n",
    "    # 3. EMPATES (TIE-BREAKING)\n",
    "    print(\"\\n[3] RIESGO DE EMPATES EN EL CORTE\")\n",
    "    # Ver si hay muchos usuarios con el mismo conteo en el borde del top 10\n",
    "    counts_values = list(counts.values())\n",
    "    counts_freq = Counter(counts_values)\n",
    "\n",
    "    # Imaginemos que el corte del top 10 es alrededor de X tweets\n",
    "    sorted_counts = sorted(counts_values, reverse=True)\n",
    "    if len(sorted_counts) > 10:\n",
    "        val_at_10 = sorted_counts[9]  # El valor del d√©cimo lugar\n",
    "        users_at_cutoff = counts_freq[val_at_10]\n",
    "        print(f\" - Valor de corte (Puesto #10): {val_at_10} tweets\")\n",
    "        print(f\" - Cu√°ntos usuarios tienen exactamente ese valor: {users_at_cutoff}\")\n",
    "\n",
    "        if users_at_cutoff > 1:\n",
    "            print(\" ‚ö†Ô∏è ALERTA CR√çTICA: Hay EMPATE en el puesto #10.\")\n",
    "            print(\n",
    "                \" -> ACCI√ìN: Tu c√≥digo DEBE tener un criterio de desempate (ej: alfab√©tico) o los tests fallar√°n aleatoriamente.\"\n",
    "            )\n",
    "\n",
    "\n",
    "def discover_anomalies(file_path, sample_size=15000):\n",
    "    print(f\"--- Iniciando An√°lisis Profundo: {file_path} (n={sample_size}) ---\")\n",
    "\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i < sample_size:\n",
    "                    data.append(line)\n",
    "                else:\n",
    "                    r = random.randint(0, i)\n",
    "                    if r < sample_size:\n",
    "                        data[r] = line\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo {file_path} no existe.\")\n",
    "        return\n",
    "\n",
    "    parsed = []\n",
    "    corrupt_lines = 0\n",
    "    for line in data:\n",
    "        try:\n",
    "            parsed.append(json.loads(line))\n",
    "        except Exception:\n",
    "            corrupt_lines += 1\n",
    "\n",
    "    if corrupt_lines:\n",
    "        print(f\"[ALERTA] Se detectaron {corrupt_lines} l√≠neas corruptas en la muestra.\")\n",
    "\n",
    "    # Normalizaci√≥n para an√°lisis\n",
    "    df = pd.json_normalize(parsed)\n",
    "\n",
    "    print(\"\\n1. INTEGRIDAD DE COLUMNAS CLAVE\")\n",
    "    columns_to_check = [\"date\", \"content\", \"mentionedUsers\", \"user.username\", \"user.id\"]\n",
    "    for col in columns_to_check:\n",
    "        if col in df.columns:\n",
    "            types = df[col].apply(lambda x: type(x).__name__).value_counts()\n",
    "            nulls = df[col].isnull().sum()\n",
    "            print(f\"- '{col}': {len(types)} tipos detectados. Nulos: {nulls}\")\n",
    "        else:\n",
    "            print(f\"- [ERROR] Columna '{col}' NO encontrada.\")\n",
    "\n",
    "    print(\"\\n2. AN√ÅLISIS DE USUARIOS (Q1 & Q3)\")\n",
    "    # Verificar si un username tiene m√∫ltiples IDs (cambio de handle)\n",
    "    user_consistency = df.groupby(\"user.username\")[\"user.id\"].nunique()\n",
    "    inconsistent = user_consistency[user_consistency > 1]\n",
    "    print(f\"- Usuarios con m√°s de un ID: {len(inconsistent)}\")\n",
    "\n",
    "    # Calcular potencial de String Interning\n",
    "    total_names = len(df[\"user.username\"])\n",
    "    unique_names = df[\"user.username\"].nunique()\n",
    "    print(\n",
    "        f\"- Ratio de Repetici√≥n de Usernames: {total_names / unique_names:.2f}x (Alto ratio justifica sys.intern)\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n3. AN√ÅLISIS DE MENCIONES (Q3)\")\n",
    "    # Validar estructura interna de mentionedUsers\n",
    "    mentions_data = df[\"mentionedUsers\"].dropna()\n",
    "    has_nested_nulls = mentions_data.apply(\n",
    "        lambda x: any(m.get(\"username\") is None for m in x if isinstance(m, dict))\n",
    "    ).sum()\n",
    "    print(f\"- Registros con listas de menciones v√°lidas: {len(mentions_data)}\")\n",
    "    print(f\"- Listas con objetos internos nulos: {has_nested_nulls}\")\n",
    "\n",
    "    print(\"\\n4. AN√ÅLISIS DE EMOJIS COMPLEJOS (Q2)\")\n",
    "    all_emojis = []\n",
    "    complex_count = 0\n",
    "    for txt in df[\"content\"].dropna():\n",
    "        # emoji_list devuelve informaci√≥n detallada de cada emoji\n",
    "        found = emoji.emoji_list(txt)\n",
    "        for e in found:\n",
    "            char = e[\"emoji\"]\n",
    "            all_emojis.append(char)\n",
    "            # Si tiene m√°s de un componente unicode o caracteres especiales de uni√≥n\n",
    "            if len(char) > 1 or \"\\u200d\" in char:\n",
    "                complex_count += 1\n",
    "\n",
    "    print(f\"- Total emojis detectados: {len(all_emojis)}\")\n",
    "    print(f\"- Emojis complejos (ZWJ/Multi-char): {complex_count}\")\n",
    "    if all_emojis:\n",
    "        print(f\"- Top 3 Emojis en muestra: {Counter(all_emojis).most_common(3)}\")\n",
    "\n",
    "    print(\"\\n5. VALORES EXTREMOS Y FECHAS\")\n",
    "    df[\"date_parsed\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    print(f\"- Rango temporal: {df['date_parsed'].min()} a {df['date_parsed'].max()}\")\n",
    "    print(f\"- Tweets por fuera de 2021: {len(df[df['date_parsed'].dt.year != 2021])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üïµÔ∏è‚Äç‚ôÇÔ∏è Deep Dive Analysis: ../farmers-protest-tweets-2021-2-4.json ---\n",
      "\n",
      "[1] AN√ÅLISIS DE TRUNCAMIENTO\n",
      " - La clave 'truncated' NO existe en este dataset (probablemente ya fue procesado/aplanado).\n",
      " - Tweets que terminan visualmente en '‚Ä¶': 4\n",
      "\n",
      "[2] AN√ÅLISIS DE RETWEETS\n",
      " - Detectados por texto ('RT @...'): 10 (0.05%)\n",
      "   -> CONCLUSI√ìN: Los Retweets est√°n presentes. Q2 y Q3 estar√°n inflados por repetici√≥n.\n",
      "\n",
      "[3] COMPARATIVA DE MENCIONES (Q3)\n",
      "An√°lisis sobre 1000 registros:\n",
      " - Casos donde Metadata detecta usuarios que Regex NO ve (Hidden/Reply): 14\n",
      "   Ejemplo Hidden -> Texto: '.@RakeshTikaitBKU ‡§¨‡•ã‡§≤‡•á- ‡§∏‡§Ç‡§∏‡§¶ ‡§ú‡§æ‡§ï‡§∞ ‡§ü‡•ç‡§∞‡•à‡§ï‡•ç‡§ü‡§∞ ‡§ö‡§≤‡§æ‡§è‡§Ç‡§ó‡•á...' | Meta: {'RakeshTikaitBKU', 'KumarKunalmedia'}\n",
      " - Casos donde Regex detecta '@' que NO son usuarios v√°lidos en Metadata: 15\n",
      "   Ejemplo Falso Positivo -> Texto: '.@RakeshTikaitBKU ‡§¨‡•ã‡§≤‡•á- ‡§∏‡§Ç‡§∏‡§¶ ‡§ú‡§æ‡§ï‡§∞ ‡§ü‡•ç‡§∞‡•à‡§ï‡•ç‡§ü‡§∞ ‡§ö‡§≤‡§æ‡§è‡§Ç‡§ó‡•á...' | Regex: {'RakeshTikaitBKU', 'kumarkunalmedia'}\n"
     ]
    }
   ],
   "source": [
    "analyze_twitter_nuances(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üî¨ An√°lisis Forense Avanzado: ../farmers-protest-tweets-2021-2-4.json ---\n",
      "\n",
      "[1] INTEGRIDAD DE ENTIDADES (Usernames)\n",
      " - Usuarios √∫nicos (Crudo): 26519\n",
      " - Usuarios √∫nicos (Normalizado + Lower): 26519\n",
      " -> OK: No se detectaron colisiones, pero es buena pr√°ctica implementarlo.\n",
      "\n",
      "[2] DISTRIBUCI√ìN DE ACTIVIDAD (Q1)\n",
      " - Top 5 Usuarios m√°s activos:\n",
      "   [('jot__b', 1019), ('rebelpacifist', 850), ('MaanDee08215437', 830), ('Gurpreetd86', 636), ('GurmVicky', 597)]\n",
      " - El 99% de usuarios tiene menos de 56 tweets.\n",
      " - El usuario #1 tiene 1019 tweets.\n",
      " ‚ö†Ô∏è ALERTA: El usuario top tiene una actividad 18.2x mayor al promedio.\n",
      " -> OBSERVACI√ìN: Probable Bot. Documentar en SOLUTION.md que esto sesga los resultados.\n",
      "\n",
      "[3] RIESGO DE EMPATES EN EL CORTE\n",
      " - Valor de corte (Puesto #10): 490 tweets\n",
      " - Cu√°ntos usuarios tienen exactamente ese valor: 1\n"
     ]
    }
   ],
   "source": [
    "advanced_analysis(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando An√°lisis Profundo: ../farmers-protest-tweets-2021-2-4.json (n=15000) ---\n",
      "\n",
      "1. INTEGRIDAD DE COLUMNAS CLAVE\n",
      "- 'date': 1 tipos detectados. Nulos: 0\n",
      "- 'content': 1 tipos detectados. Nulos: 0\n",
      "- 'mentionedUsers': 2 tipos detectados. Nulos: 10111\n",
      "- 'user.username': 1 tipos detectados. Nulos: 0\n",
      "- 'user.id': 1 tipos detectados. Nulos: 0\n",
      "\n",
      "2. AN√ÅLISIS DE USUARIOS (Q1 & Q3)\n",
      "- Usuarios con m√°s de un ID: 0\n",
      "- Ratio de Repetici√≥n de Usernames: 2.19x (Alto ratio justifica sys.intern)\n",
      "\n",
      "3. AN√ÅLISIS DE MENCIONES (Q3)\n",
      "- Registros con listas de menciones v√°lidas: 4889\n",
      "- Listas con objetos internos nulos: 0\n",
      "\n",
      "4. AN√ÅLISIS DE EMOJIS COMPLEJOS (Q2)\n",
      "- Total emojis detectados: 5432\n",
      "- Emojis complejos (ZWJ/Multi-char): 1191\n",
      "- Top 3 Emojis en muestra: [('üôè', 616), ('üòÇ', 426), ('üöú', 321)]\n",
      "\n",
      "5. VALORES EXTREMOS Y FECHAS\n",
      "- Rango temporal: 2021-02-12 01:36:50+00:00 a 2021-02-24 09:23:10+00:00\n",
      "- Tweets por fuera de 2021: 0\n"
     ]
    }
   ],
   "source": [
    "discover_anomalies(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definici√≥n de estaratefias de optimizaci√≥n\n",
    "\n",
    "Depues de analizar los datos y el c√≥digo,ver detalle en la secci√≥n 3 del archivo [SOLUTION.md](../SOLUTION.md) \n",
    "\n",
    "\n",
    "## Calidad de software\n",
    "\n",
    "Se proponen realizar test basados en los resultados del analisis de los datos. ver detalle en la secci√≥n 4 del archivo [SOLUTION.md](../SOLUTION.md.\n",
    "\n",
    "\n",
    "## C√≥digo\n",
    "\n",
    "El proceso de optimizaci√≥n de el procesamiento de datos empieza desde el proceso de lectura\n",
    "del archivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ INICIANDO BENCHMARK INTEGRAL DESACOPLADO: ../farmers-protest-tweets-2021-2-4.json\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Standard JSON\n",
      "######################################################################\n",
      "\n",
      "[PERF] read_standard_json:\n",
      "  > Tiempo: 13.0648 s\n",
      "  > Memoria: 1126.69 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: read_standard_json ---\n",
      "         1391837 function calls (1391820 primitive calls) in 12.796 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 154 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        7    0.000    0.000   12.791    1.827 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:319(select)\n",
      "        7    0.000    0.000   12.791    1.827 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:313(_select)\n",
      "   117407    0.220    0.000    9.775    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\__init__.py:299(loads)\n",
      "   117407    0.811    0.000    9.455    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\decoder.py:333(decode)\n",
      "        6    1.493    0.249    8.964    1.494 {built-in method select.select}\n",
      "   117407    8.240    0.000    8.240    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\decoder.py:344(raw_decode)\n",
      "        1    0.618    0.618    3.826    3.826 C:\\Users\\johanmarin\\AppData\\Local\\Temp\\ipykernel_11928\\2840695738.py:19(read_standard_json)\n",
      "    49772    0.751    0.000    0.861    0.000 <frozen codecs>:319(decode)\n",
      "   234814    0.277    0.000    0.277    0.000 {method 'match' of 're.Pattern' objects}\n",
      "    49772    0.110    0.000    0.110    0.000 {built-in method _codecs.utf_8_decode}\n",
      "   234814    0.089    0.000    0.089    0.000 {method 'end' of 're.Match' objects}\n",
      "   117407    0.068    0.000    0.068    0.000 {method 'startswith' of 'str' objects}\n",
      "   117412    0.045    0.000    0.045    0.000 {method 'append' of 'list' objects}\n",
      "   117433    0.039    0.000    0.039    0.000 {built-in method builtins.len}\n",
      "117564/117558    0.033    0.000    0.033    0.000 {built-in method builtins.isinstance}\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: read_standard_json ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Streaming Orjson\n",
      "######################################################################\n",
      "\n",
      "[PERF] read_streaming_orjson:\n",
      "  > Tiempo: 0.0000 s\n",
      "  > Memoria: 173.15 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: read_streaming_orjson ---\n",
      "         1 function calls in 0.000 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: read_streaming_orjson ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Chunks Orjson\n",
      "######################################################################\n",
      "\n",
      "[PERF] read_chunks_orjson:\n",
      "  > Tiempo: 0.0000 s\n",
      "  > Memoria: 173.15 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: read_chunks_orjson ---\n",
      "         2 function calls in 0.000 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\johanmarin\\AppData\\Local\\Temp\\ipykernel_11928\\2840695738.py:39(read_chunks_orjson)\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: read_chunks_orjson ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Full Memory (readlines)\n",
      "######################################################################\n",
      "\n",
      "[PERF] read_full_mem_json:\n",
      "  > Tiempo: 12.0362 s\n",
      "  > Memoria: 1525.38 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: read_full_mem_json ---\n",
      "         1391914 function calls (1391902 primitive calls) in 12.527 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 156 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        9    0.130    0.014   13.308    1.479 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:319(select)\n",
      "   117407    0.200    0.000    9.358    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\__init__.py:299(loads)\n",
      "   117407    1.385    0.000    9.069    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\decoder.py:333(decode)\n",
      "   117407    7.317    0.000    7.317    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\decoder.py:344(raw_decode)\n",
      "        8    0.000    0.000    1.476    0.184 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:313(_select)\n",
      "        8    1.423    0.178    1.475    0.184 {built-in method select.select}\n",
      "        1    0.030    0.030    1.197    1.197 C:\\Users\\johanmarin\\AppData\\Local\\Temp\\ipykernel_11928\\2840695738.py:44(read_full_mem_json)\n",
      "        1    1.058    1.058    1.147    1.147 {method 'readlines' of '_io._IOBase' objects}\n",
      "   234814    0.242    0.000    0.242    0.000 {method 'match' of 're.Pattern' objects}\n",
      "   117407    0.206    0.000    0.206    0.000 {method 'strip' of 'str' objects}\n",
      "    49772    0.047    0.000    0.141    0.000 <frozen codecs>:319(decode)\n",
      "    49772    0.094    0.000    0.094    0.000 {built-in method _codecs.utf_8_decode}\n",
      "   234814    0.088    0.000    0.088    0.000 {method 'end' of 're.Match' objects}\n",
      "   117407    0.060    0.000    0.060    0.000 {method 'startswith' of 'str' objects}\n",
      "   117437    0.037    0.000    0.037    0.000 {built-in method builtins.len}\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: read_full_mem_json ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Pandas read_json\n",
      "######################################################################\n",
      "\n",
      "[PERF] read_pandas_json:\n",
      "  > Tiempo: 15.1821 s\n",
      "  > Memoria: 2654.88 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: read_pandas_json ---\n",
      "         486596 function calls (486235 primitive calls) in 14.795 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 549 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.125    0.125   14.592   14.592 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:505(read_json)\n",
      "        6    0.000    0.000    8.293    1.382 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:319(select)\n",
      "        6    0.006    0.001    8.292    1.382 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:313(_select)\n",
      "        6    0.986    0.164    8.277    1.379 {built-in method select.select}\n",
      "        1    7.291    7.291    7.291    7.291 {built-in method pandas._libs.json.ujson_loads}\n",
      "        1    0.068    0.068    3.132    3.132 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:991(read)\n",
      "        1    0.000    0.000    3.064    3.064 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1022(_get_object_parser)\n",
      "        1    0.000    0.000    3.064    3.064 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1174(parse)\n",
      "        3    0.000    0.000    2.067    0.689 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:698(__init__)\n",
      "        1    0.257    0.257    1.894    1.894 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1386(_parse)\n",
      "        1    0.000    0.000    1.375    1.375 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:506(nested_data_to_arrays)\n",
      "        1    0.004    0.004    1.375    1.375 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:793(to_arrays)\n",
      "       18    1.167    0.065    1.167    0.065 {method 'split' of 'str' objects}\n",
      "        1    0.035    0.035    1.162    1.162 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1452(_try_convert_types)\n",
      "        2    0.000    0.000    1.081    0.541 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1422(_process_converter)\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: read_pandas_json ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Polars read_ndjson (Eager)\n",
      "######################################################################\n",
      "\n",
      "[PERF] read_polars_json:\n",
      "  > Tiempo: 8.8614 s\n",
      "  > Memoria: 2651.02 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: read_polars_json ---\n",
      "         515 function calls (511 primitive calls) in 8.256 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 170 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      2/1    0.000    0.000    8.255    8.255 C:\\Users\\johanmarin\\AppData\\Local\\Temp\\ipykernel_11928\\2840695738.py:54(read_polars_json)\n",
      "        1    0.202    0.202    8.255    8.255 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\io\\ndjson.py:25(read_ndjson)\n",
      "        3    0.000    0.000    4.627    1.542 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\asyncio\\base_events.py:1922(_run_once)\n",
      "        3    0.000    0.000    4.626    1.542 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:319(select)\n",
      "        3    0.000    0.000    4.626    1.542 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\selectors.py:313(_select)\n",
      "        3    4.626    1.542    4.626    1.542 {built-in method select.select}\n",
      "        1    0.000    0.000    3.425    3.425 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:84(wrapper)\n",
      "        1    0.000    0.000    3.425    3.425 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:308(wrapper)\n",
      "        1    0.000    0.000    3.425    3.425 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2198(collect)\n",
      "        1    3.425    3.425    3.425    3.425 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "        4    0.000    0.000    0.002    0.000 C:\\Users\\johanmarin\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\asyncio\\events.py:86(_run)\n",
      "        4    0.000    0.000    0.002    0.000 {method 'run' of '_contextvars.Context' objects}\n",
      "        1    0.000    0.000    0.001    0.001 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\tornado\\ioloop.py:750(_run_callback)\n",
      "        1    0.000    0.000    0.001    0.001 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\ipykernel\\iostream.py:611(_flush)\n",
      "        1    0.000    0.000    0.001    0.001 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py:206(_handle_events)\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: read_polars_json ---\n",
      "\n",
      "\n",
      "######################################################################\n",
      "### ESTRATEGIA: Polars scan_ndjson (Lazy)\n",
      "######################################################################\n",
      "\n",
      "[PERF] read_polars_lazy:\n",
      "  > Tiempo: 0.0002 s\n",
      "  > Memoria: 2285.50 MB\n",
      "\n",
      "--- INICIANDO PERFILADO DETALLADO: read_polars_lazy ---\n",
      "         33 function calls (32 primitive calls) in 0.000 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 27 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\johanmarin\\AppData\\Local\\Temp\\ipykernel_11928\\2840695738.py:58(read_polars_lazy)\n",
      "      2/1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:123(wrapper)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\io\\ndjson.py:177(scan_ndjson)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\io\\cloud\\credential_provider\\_builder.py:303(_init_credential_provider_builder)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\logging.py:7(verbose)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen os>:808(getenv)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen _collections_abc>:804(get)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\io\\cloud\\credential_provider\\_builder.py:312(f)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\various.py:232(normalize_filepath)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method new_from_ndjson}\n",
      "        1    0.000    0.000    0.000    0.000 <frozen os>:709(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\_utils\\wrap.py:16(wrap_ldf)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen ntpath>:351(expanduser)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\johanmarin\\Documents\\challenge_DE\\.venv\\Lib\\site-packages\\polars\\lazyframe\\frame.py:429(_from_pyldf)\n",
      "\n",
      "\n",
      "\n",
      "--- FIN PERFILADO DETALLADO: read_polars_lazy ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from benchmark import profile_performance, profile_detailed\n",
    "\n",
    "\n",
    "def chunk_generator(file_path: str, chunk_size: int = 5000) -> Iterable[List[dict]]:\n",
    "    \"\"\"Devuelve un generador de listas de diccionarios (chunks).\"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            try:\n",
    "                chunk.append(orjson.loads(line))\n",
    "                if len(chunk) >= chunk_size:\n",
    "                    yield chunk\n",
    "                    chunk = []\n",
    "            except orjson.JSONDecodeError:\n",
    "                continue\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "\n",
    "\n",
    "def read_standard_json(file_path: str) -> List[dict]:\n",
    "    \"\"\"Lee y devuelve una lista completa de diccionarios (Standard).\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_streaming_orjson(file_path: str) -> Iterable[dict]:\n",
    "    \"\"\"Devuelve un generador (Lazy) de diccionarios usando orjson.\"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                yield orjson.loads(line)\n",
    "            except orjson.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "\n",
    "def read_chunks_orjson(file_path: str) -> Iterable[List[dict]]:\n",
    "    \"\"\"Llamada directa al generador de chunks.\"\"\"\n",
    "    return chunk_generator(file_path, 5000)\n",
    "\n",
    "\n",
    "def read_full_mem_json(file_path: str) -> List[dict]:\n",
    "    \"\"\"Lee todo el archivo en memoria y devuelve la lista de diccionarios.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [json.loads(line) for line in lines if line.strip()]\n",
    "\n",
    "\n",
    "def read_pandas_json(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Lee y devuelve un DataFrame de Pandas.\"\"\"\n",
    "    return pd.read_json(file_path, lines=True)\n",
    "\n",
    "\n",
    "def read_polars_json(file_path: str) -> pl.DataFrame:\n",
    "    \"\"\"Lee y devuelve un DataFrame de Polars (Eager).\"\"\"\n",
    "    return pl.read_ndjson(file_path, infer_schema_length=None, ignore_errors=True)\n",
    "\n",
    "\n",
    "def read_polars_lazy(file_path: str) -> pl.LazyFrame:\n",
    "    \"\"\"Devuelve un LazyFrame de Polars.\"\"\"\n",
    "    return pl.scan_ndjson(file_path, infer_schema_length=None, ignore_errors=True)\n",
    "\n",
    "\n",
    "def run_benchmarks(file_path: str):\n",
    "    print(f\"üöÄ INICIANDO BENCHMARK INTEGRAL DESACOPLADO: {file_path}\\n\")\n",
    "\n",
    "    strategies = [\n",
    "        (\"Standard JSON\", read_standard_json),\n",
    "        (\"Streaming Orjson\", read_streaming_orjson),\n",
    "        (\"Chunks Orjson\", read_chunks_orjson),\n",
    "        (\"Full Memory (readlines)\", read_full_mem_json),\n",
    "        (\"Pandas read_json\", read_pandas_json),\n",
    "        (\"Polars read_ndjson (Eager)\", read_polars_json),\n",
    "        (\"Polars scan_ndjson (Lazy)\", read_polars_lazy),\n",
    "    ]\n",
    "\n",
    "    for name, func in strategies:\n",
    "        print(f\"\\n{'#' * 70}\")\n",
    "        print(f\"### ESTRATEGIA: {name}\")\n",
    "        print(f\"{'#' * 70}\")\n",
    "\n",
    "        # Medici√≥n 1: Rendimiento Real (Tiempo Wall-clock + Pico RAM)\n",
    "        # Esto es lo que realmente importa para la eficiencia base\n",
    "        perf_monitor = profile_performance(func)\n",
    "        perf_monitor(file_path)\n",
    "\n",
    "        # Medici√≥n 2: An√°lisis T√©cnico (Opcional - Reporte cProfile)\n",
    "        # Se ejecuta aparte para no influir en los tiempos reales de arriba\n",
    "        detailed_monitor = profile_detailed(func)\n",
    "        detailed_monitor(file_path)\n",
    "\n",
    "\n",
    "run_benchmarks(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
